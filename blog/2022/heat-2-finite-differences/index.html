<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="-mRHvwnHdFYIa-KaKIgZIGLfsdeDLGZ9rZEsWECSiuE"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Heat #2 - Solving the Heat Equation with Finite-Differences | Manuel Madeira</title> <meta name="author" content="Manuel Madeira"> <meta name="description" content="Second blogpost of the Heat series, on how to solve the Heat equation using a Finite-Differences method."> <meta name="keywords" content="Manuel, Madeira, personal, web, site, page, machine learning, artificial intelligence"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://manuelmlmadeira.github.io/blog/2022/heat-2-finite-differences/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Manuel <span class="font-weight-bold">Madeira</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Heat #2 - Solving the Heat Equation with Finite-Differences</h1> <p class="post-meta">March 3, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </header> <article class="post-content"> <p><strong>Authors</strong>: Manuel Madeira, David Carvalho</p> <p><strong>Reviewers</strong>: Fábio Cruz, Augusto Peres</p> <p><em>This blog post was developed while working at <a href="https://inductiva.ai/" rel="external nofollow noopener" target="_blank">Inductiva Research Labs</a>.</em></p> <hr> <h2 id="discretizing-our-domain">Discretizing our domain</h2> <p>Deciding how to discretize the domain is also <em>far</em> from being set on stone: for a given instantiation of a PDE problem, it is typically one of the most complex steps in its resolution. Squares are cool and all but what if we want to simulate the Heat Equation in a mug or a pan?</p> <p>We’ll make our lives easier (for now!) by using a <em>regular grid</em> to represent the domain. With the aid of the cube:</p> \[(t,x,y) \in [t_\mathrm{min}, t_\mathrm{max}] \times [x_\mathrm{min}, x_\mathrm{max}] \times [y_\mathrm{min}, y_\mathrm{max}]\] <p>in a regular grid with $N_t$, $N_x$ and $N_y$ points along the $t$, $x$ and $y$-axis, respectively, we can set their step intervals, defined by their regular spacing along their respective dimension:</p> \[(\Delta t, \Delta x, \Delta y) = \left( \frac{t_\mathrm{max} - t_\mathrm{min}}{N_t-1}, \frac{x_\mathrm{max} - x_\mathrm{min}}{N_x-1}, \frac{y_\mathrm{max} - y_\mathrm{min}}{N_y-1} \right)\] <p>Consequently, input points $(t,x,y)$ become discretized as $(t_k, x_i, y_j)$ and associated with a node $[k,i,j]$. Here,</p> \[(t_k, x_i, y_j) = \left\{ \begin{matrix} t_k = t_0 + k \Delta t \ \ \ \ \ \ \ \ \ \ \ \ \text{for} \ 0 \leq k \leq N_t-1 \\ x_i = x_0 + i \Delta x \ \ \ \ \ \ \ \ \ \ \ \text{for} \ \ 0 \leq i \leq N_x-1 \\ y_j = y_0 + j \Delta y \ \ \ \ \ \ \ \ \ \ \ \text{for} \ 0 \leq j \leq N_y-1 \end{matrix} \right.\] <p>It is in this <em>pixelated</em> world we will express how heat will diffuse away…</p> <h2 id="from-continuous-to-discrete-derivatives">From continuous to discrete derivatives</h2> <p>So, we now need to express a differential operator in a finite, discretized domain. How exactly do we <em>discretize</em> such abstract objects, like the (univariate) derivative $f_x(x)$:</p> \[f_x(x) = \underset{\Delta x \to 0}{\mathrm{lim}} \; \frac{f(x+ \Delta x) - f(x)}{\Delta x}.\] <p>or a partial derivative with respect to, say, a coordinate $x_j$:</p> \[f_{x_j}(x_1, \dots, x_j,\ldots,x_N)= \underset{\Delta x_j \to 0}{\mathrm{lim}} \; \frac{f(x_1, \dots, x_j + \Delta x_j, \dots ,x_N) - f(x_1, \ldots,x_j, \dots, x_N)}{\Delta x_j}\] <p>Being on a grid means that we require information on a <em>finite</em> number of points in the domain. The FDM philosophy is to express (infinitesimal) differentials by <strong>(finite) differences</strong> between any nodes.</p> <p>The <em>continuous</em> thus becomes <em>discrete</em> — the differential operator $\mathcal{L}(t,x,y)$ becomes a function-valued discrete <em>grid</em> $\equiv \mathcal{L}[k,i,j] = \mathcal{L}(t_k,x_i,y_j) $. Resulting from this, so will our solution be output as the grid $u[k,i,j]$.</p> <p>In essence, we must find an algorithm which can propagate the constrains set by the PDE from the initial conditions as faithfully as possible along the grid. In a more specific sense, you might wonder:</p> <blockquote> <p>How to approximate $\mathcal{L}$ <em>reasonably</em> to obtain $u[k,i,j]$ for all nodes $[k,i,j]$?</p> </blockquote> <p>This is exactly what the FDM solver will accomplish. For that, it has to approximate the differential operators — $u_{t}[k,i,j]$, $u_{xx}[k,i,j]$ and $u_{yy}[k,i,j]$ — <strong>at all nodes</strong> $[k,i,j]$.</p> <p><em>Goodbye operators, hello grids!</em></p> <h2 id="finite-difference-method">Finite Difference Method</h2> <p>Approximating differentials on a discrete set is also not a recipe set on stone. Let us look at two Finite Difference approximations to a function of a single variable $x$:</p> <div style="text-align: center;"> <img style="width:40%;" src="../../../assets/img/blogposts/heat2/derivatives.png"> </div> <p><strong>Fig. 1</strong>: Approximation schemes of the derivative of $f(x)$ at a grid point $x_i$ using - (left) <em>forward</em> and (right) <em>centered</em> schemes. Note that either method differs from the <em>actual</em> value $f_x(x_i)$, given by the slope of the solid straight line. Credits: David Carvalho / Inductiva.</p> <p>These will estimate the true instantaneous rate via neighboring differences. Expectedly, an error will be incurred in the process.</p> <p>The price to pay in “dropping” the limit results in a 1st-order approximation, meaning this estimate scales <em>linearly</em> with the spacing $\Delta x$. For our approximation to have a chance to succeed, we better sample the $x$ axis with a high $N_x$!</p> <h2 id="choosing-how-to-spread-heat">Choosing how to spread heat</h2> <p>Given a particular PDE, different FDMs would iterate differently over the grid <strong>node by node</strong> to obtain estimates of the differential operators. <br> So, fixing <em>a scheme</em> is (again) delicate task.</p> <p>You guessed it right - for this problem, we will use the 2 approximation schemes we showed before to evaluate the differential operators. They are combined in the <em>FTCS scheme</em>:</p> <ul> <li> <p><strong>Forward in Time</strong> - the time partial derivative $u_t$ uses the <em>forward</em> 1st order differences</p> \[u^{k, i, j}_t = \frac{u^{k+1, i, j} - u^{k, i, j}}{\Delta t}\] </li> <li> <p><strong>Centered in Space</strong> - the spatial partial derivatives $u_{xx}$ and $u_{yy}$ are are computed through a 2nd-order <em>central difference</em> by applying the centered difference <em>twice</em>: - We first approximate the (second-order) derivative (say, $u_{xx}$) via the centered difference:</p> \[u^{k, i, j}_{xx} = \frac{u^{k, i+\frac{1}{2}, j}_x - u^{k, i-\frac{1}{2}, j}_x}{\Delta x}\] <ul> <li>And then express each first-order term $u_x$ in the same fashion e.g.:</li> </ul> \[u^{k, i, j}_{x} = \frac{u^{k, i+\frac{1}{2}, j} - u^{k, i-\frac{1}{2}, j}}{\Delta x}\] <ul> <li>With a bit of rearranging, both spatial derivatives become:</li> </ul> \[\begin{split} u_{xx}^{k,i,j} = \frac{u^{k, i+1, j} - 2 u^{k, i, j} + u^{k, i-1, j}}{(\Delta x)^2} &amp;&amp; \ \ \ \ \ \ u_{yy}^{k,i,j}= \frac{u^{k,i,j+1} - 2 u^{k,i,j} + u^{k,i,j-1}}{(\Delta y)^2} \end{split}\] </li> </ul> <p>Look — some terms are evaluated <em>outside</em> the original grid. However, you will notice that these fractional indexes only appear for the computation of intermediary constructions/variables [4].</p> <h2 id="heat-diffusion-on-a-grid">Heat diffusion on a grid</h2> <p><em>Phew</em>! That was intense but we now have all the tools to start simulating heat flow! For that, we must:</p> <ol> <li> <em>discretize the domain</em> by sampling each dimension individually and considering all possible combinations of the coordinates, creating points $(t_k,x_i,y_j)$, indexed by nodes $[k,i,j]$.</li> <li> <em>discretize</em> the differential operators using a FTCS scheme.</li> <li>solve for the solution grid $u[k,i,j]$ by using the <em>iteration rules</em> that propagate the solution across all nodes $[k,i,j]$.</li> </ol> <p>In this fashion, we convert the 2D Heat Equation to this <strong>difference equation</strong>:</p> \[u^{k+1, i, j} = \alpha (u^{k, i+1, j} + u^{k, i-1, j}) + \beta (u^{k, i, j+1} + u^{k, i, j-1}) + (1 - 2\alpha - 2\beta) u^{k, i, j}\] <p>where we can lump all parameters in the ratios:</p> \[\begin{split} \alpha \equiv D\frac{\Delta t}{(\Delta x)^2} \ \ \ \ \ \ &amp;&amp; \ \ \ \ \ \ \ \beta \equiv D \frac{\Delta t}{(\Delta y)^2}. \end{split}\] <p>We can understand how knowledge about the function <strong>propagates</strong> along the grid with a <em>stencil</em> by depicting which input grid points are needed to iterate the algorithm so a solution estimate may be computed at all grid points.</p> <div style="text-align: center;"> <video class="mb-0" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/fdm_animation.mp4" type="video/mp4"></source> </video> </div> <p><strong>Fig. 2</strong>: A stencil used by the FTCS scheme. At a given node $[k,i,j]$, the solution is propagated forward in time as $u[k+1,i,j]$ by considering the 2-point centered derivatives in $x$ (the neighboring nodes $[k,i+1,j]$ and $[k,i-1,j]$) and in $y$ (the nodes $[k,i,j+1]$ and $[k,i,j-1]$). Credits: Augusto Peres, Inductiva.</p> <h2 id="run-our-code">Run our code</h2> <p>It’s time to play with all these concepts and see how our FDM approximation fares.</p> <p><strong>All our code can be accessed on our <a href="https://github.com/inductiva/blog_code_snippets" rel="external nofollow noopener" target="_blank">code snippets GitHub repository</a></strong>. There, you will find the <code class="language-plaintext highlighter-rouge">heat_fdm.py</code> file for simulations and plotting utilities in <code class="language-plaintext highlighter-rouge">/utils</code>. <br> Have a go yourself!</p> <h2 id="the-role-of-the-d-ffusivity">The role of the $D$-ffusivity</h2> <p>We will pick a spatial discretization of, say, $500 \times 500$ points. We can now run for different thermal diffusivities and see their effect. Below you can see the temperature profiles as we increase $D$ — first with $D=0.01$, then $D=0.1$ and finally $D=1$.</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/ftcs_d001.mp4" type="video/mp4"></source> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/ftcs_d01.mp4" type="video/mp4"></source> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/ftcs_d1.mp4" type="video/mp4"></source> </video> </div> <p><strong>Fig. 3</strong>: Role of various $D$ in the diffusion. Credits: Manuel Madeira / Inductiva.</p> <h2 id="heat-is-on-the-plate">Heat is on the plate!</h2> <p>We can reason with these results.</p> <ul> <li> <p>At the beginning (for $t=0$), heat starts developing from the only points that are not at the common temperature ($-1 \; ^\mathrm{o}C$), where temperature gradients exist. These are points nearby the hot (top) edge.</p> </li> <li> <p>From that point on, we can observe the progressive heat diffusion from the hot edge towards lower regions, until we reach a stationary state, where no further changes occur (and the system is in <strong>thermal equilibrium</strong>).</p> </li> <li> <p>Heat does not spread evenly across the plate — its spread is faster in regions that are further from the cold regions. Given this symmetric setup, points along the central line between the edges allow for faster diffusion than regions nearby the edges. This will progressively wind down until we reach the cold edges and a paraboloid-like pattern is observed.</p> </li> <li> <p>But more importantly — the higher the $D$, the faster this spread occurs. This <strong>directly</strong> impacts the choice of the time sampling.</p> </li> </ul> <p>So — it seems that each <strong>internal</strong> parameter $D$ requires its own discretization setup somehow. But in what way?</p> <h2 id="setting-stability-criteria">Setting stability criteria</h2> <p>We ran our first PDE classical solver. <em>Yay!</em> However, why are we happy with the results? Clearly, we can think of potential issues, such as:</p> <ul> <li>for excessively sparse discretizations, our approximations of the differential operators will be miserably <em>undersampled</em> and consequently the trial solution will be <em>shockingly wrong</em>.</li> <li> <strong>error buildup</strong> will take place as the iterative procedure tends to propagate and amplify errors between any iterations. In those cases, we say that the method is <strong>unstable</strong>.</li> </ul> <p>However, these are the extreme instances. Were we lucky? If we are using “reasonable” discretizations, a question should make you scratch your head:</p> <blockquote> <p>How to <em>be sure</em> a certain discretization provides a trustful approximation to the PDE solution?</p> </blockquote> <p>Fortunately for us, you can notice that the discretization setup can be monitored <strong>exclusively</strong> through the ratios $\alpha$ and $\beta$! This raises the question:</p> <blockquote> <p>Can we somehow fine-tune them to ensure <em>stability</em> <em>i.e.</em> find admissible combinations of $(\Delta t, \Delta x, \Delta y)$ for a fixed $D$?</p> </blockquote> <p>Well, theoretical studies can be performed to find a restricted space of acceptable discretization setups. For our case [3], <em>Von-Neumann analysis</em> establishes a surprisingly simple <em>stability criterion</em>:</p> \[\alpha + \beta \leq \frac{1}{2}\] <p>These two constants provide a straightforward procedure to be sure that we i) are not <strong>oversampling</strong> or <strong>undersampling</strong> a dimension with respect to any other and ii) have spacings that can capture meaningful changes of the solution.</p> <p><strong>This is not a loose condition</strong>. <br> Just to show you that we’re playing with fire, we cannot think of a more illustrative example than by ramping up that bound by a mere $2 \% $ above the theoretical maximum <em>i.e.</em> for $\alpha + \beta \approx 0.51$:</p> <div style="text-align: center;"> <video style="width:80%;" class="mb-0" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/ftcs_unstable.mp4" type="video/mp4"></source> </video> </div> <p><strong>Fig. 4</strong>: In a discretization step for which $\alpha + \beta &gt; 0.5$, our algorithm is <em>bound to fail</em>. Credits: Manuel Madeira / Inductiva</p> <p>Yup: for this setup, an “explosion” caused by the FDM instability can be seen propagating along the trial solution — quickly unfolding to some psychedelic burst!</p> <p>At the very beginning, some similarities to the downwards diffusion pattern ( that we just saw previously) quickly <strong>fade away</strong> as the <strong>error buildup spreads</strong> away and <strong>faster</strong> in regions with already large errors.</p> <p>It’s a race for disaster: the exponential buildup of the errors, even if sustained for only a handful of propagation steps, ensures that some nodes will experience <em>huge growth</em> (and $u$ becomes <em>unbounded</em>) — $ u \mapsto 1 \mapsto 10^{15} \mapsto \dots 10^{80} \dots $, until the maximum computer precision is reached — at which point its value becomes a <code class="language-plaintext highlighter-rouge">NaN</code> (Not a Number). You can see that eventually <strong>all</strong> points are not even represented on the scale (they’re white)!</p> <p>By the same token, let’s now see what happens if we’re <em>close to the edge</em> from within the admissible area. Dropping our bound by $2 \% $, let’s heat the start button for $\alpha + \beta \approx 0.49$.</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/ftcs_almost_unstable.mp4" type="video/mp4"></source> </video> </div> <p><strong>Fig. 5</strong>: With our parameters recalibrated, stability has been restored! Credits: Manuel Madeira / Inductiva</p> <p><em>Incredible!</em> Not even a minimal sign of instability coming through.</p> <p>We’re happy - we obtained seemingly satisfactory outputs in a reasonable way. The simulations take about $10s$ and CPUs can handle the computation just fine.</p> <p>But remember though: this is an extremely simple system… We could be considering this problem in 3D or at way more complex domains and boundaries. In an extreme limit, we could be using these FDMs in highly-nonlinear equations with hundreds of variables.</p> <h2 id="to-vectorize-or-not-to-vectorize--that-is-not-the-question"> <em>To Vectorize or Not To Vectorize</em> – that is not the question</h2> <p>Understandingly, instructing how to compute many functions over a grid is not a trivial task — issues like computation power and memory storage can be bottlenecks already for discretization setups <em>far from ideal</em>.</p> <p>We must understand how our implementation choices affect the algorithm performance and the output accuracy. In essence:</p> <blockquote> <p>How much faster can we solve the PDE without compromising the output?</p> </blockquote> <p>Using the <em>running time</em> as a metric, we’ll compare 3 mainstream implementations:</p> <ul> <li> <p><strong>Nested <code class="language-plaintext highlighter-rouge">For</code> Cycles</strong>: we fill in the entries for the plate at the new time frame $u^{k+1, i, j}$, by nesting 2 <code class="language-plaintext highlighter-rouge">For</code> cycles that iterate through the $x$ and $y$ axes from the previous time frame in $u^{k, i, j}$.</p> </li> <li> <p><strong>Vectorized approach</strong>: we perform arithmetic operations on $u^{k+1,i,j}$ by considering it as a linear combination of slightly shifted versions $u^{k,i,j}$, multiplied by constants (known <em>a priori</em>), resulting in scalar multiplications and additions of matrices. <em>NumPy</em> implementations will be used.</p> </li> <li> <p><strong>Compiled approach</strong>: we can compile a vectorized approach, which allows sequences of operations to be optimized together and run at once. The code compilation was performed through <strong>JAX</strong>, using its just-in-time (JIT) compilation decorator.</p> </li> </ul> <p>The approach used is passed as a flag: <code class="language-plaintext highlighter-rouge">--vectorization_strategy=my_strategy</code>. <br> The default is set to <code class="language-plaintext highlighter-rouge">numpy</code> and the vectorized approach is used. When set to <code class="language-plaintext highlighter-rouge">jax</code>, a compiled version of the vectorized approach is used and, when set to <code class="language-plaintext highlighter-rouge">serial</code>, the nested <em>For</em> cycles approach is used.</p> <p>After running each implementation for the same parameters, we obtain <em>wildly</em> different running times:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vectorization_strategy=serial - Time spent in FDM: 93.7518858909607s
vectorization_strategy=numpy - Time spent in FDM: 0.6520950794219971s
vectorization_strategy=jax - Time spent in FDM: 0.297029972076416s
</code></pre></div></div> <p><em>Wow!</em> By using the vectorized approach, we obtain more than a $100$-fold boost in running speed!</p> <p>This is not surprising. The vectorized approach takes advantage <strong>SIMD</strong> (Single Instruction, Multiple Data) computations, where one instruction carries out the same operation on a number of operands in parallel.</p> <p>This discrepancy is then easy to understand and a huge acceleration is observed, in constrast when ran with the nested <code class="language-plaintext highlighter-rouge">For</code> cycles, where each instruction is executed <strong>one at a time</strong>.</p> <p>We are not done yet. By compiling the vectorized approach, we obtain yet another $3$-fold speed-up, further improving our metric! This improvement would be even more significant if we ran the compiled code in GPUs (currently, JAX still only supports CPU in Apple Silicon machines).</p> <p>Computation resources are finite (and costly). Despite the huge boost granted by the compiled vectorized approach, this methodology improved our task by 2 orders of magnitude. Is that enough when dealing with far more complex PDEs in higher dimensions?</p> <h2 id="how-far-can-we-push-classical-methods">How far can we push classical methods?</h2> <p>Despite our pleasant experience with a low-dimensional problem on a regular geometry, we should be careful thinking ahead. Classical methods for solving PDEs have long been recognized for suffering from some significant drawbacks and we will comment on a few:</p> <h2 id="meshing">Meshing</h2> <p>Meshing becomes a daunting task in more complex geometries. Different discretization setups can always be chosen but once one is fixed, we have limited ourselves to computations <strong>on the nodes generated by that particular setup</strong>. We can no longer apply <em>arbitrary smaller</em> steps to approximate the differential operators.</p> <p>Worse still — depending on the behavior of the differential operator, we might need to focus on certain regions more than others. It’s unclear how these choices impact the quality of the approximations.</p> <p>Discretizing the problem also creates a bias in favor of the grid points with respect to all others.</p> <blockquote> <p>But what if there the PDE behaves differently outside the mesh?</p> </blockquote> <p>The best we can do is to infer a solution through <strong>interpolation</strong> but this is admittedly <strong>unsatisfactory</strong>.</p> <h3 id="scaling">Scaling</h3> <p>Classical algorithms scale <strong>terribly</strong> with the PDE dimensions and grid refinements.</p> <p>For all the Complexity lovers out there — you know that sweeping the regular grid $[k,i,j]$ step-by-step requires algorithms of order $\mathcal{O}(N_t N_x N_y)$. <br> By simply refining our grid by $5$ times, we scale the number of nodes by a whopping factor $5^3 = 125$ (!!!).</p> <p>Similar bad news are expected as we consider simulations on PDEs for higher dimensions. For a problem of 3D heat diffusion, the complexity grows as $\mathcal{O} (N_t N_x N_y N_z)$ and naturally, $n$ dimensions scale as $\mathcal{O}(N^d)$ — an <em>exponential scaling</em> of the algorithm with dimension.</p> <h2 id="feasibility">Feasibility</h2> <p>Even though we can rely on stability conditions, this does <strong>not</strong> mean we can deploy them — it may be computationally very, very, …, heavy or simply impossible.</p> <p>As we saw, simulating a medium with <em>higher</em> $D$ requires a <em>finer</em> time discretization. For some instances, satisfactory $\Delta t$ might be too small to be practical and, in the extreme case, downright untenable!</p> <p>This comes at a bitter price and is a <strong>major</strong> drawback of the FDM.</p> <h2 id="machine-learning">Machine Learning?</h2> <p>PDE simulations must be <strong>as fast and light as possible</strong>. But can classical methods, in general, get <em>fast enough</em>?</p> <p>We can look farther ahead. Consider something extreme (but likely). Imagine our task is to find parameters that lead to optimal performance for some problem.</p> <p>Let us assume we have access to the theoretically fastest PDE classical solver, foolproof to instabilities and extremely efficient in terms of memory and processing.</p> <blockquote> <p>If hundreds, millions, …, billions of intermediate PDE calculations had to be performed with that algorithm, could we afford the computational budget? Would the time elapsed be reasonable?</p> </blockquote> <p><strong>Certainly not.</strong> It is not difficult to estimate runtimes <em>comparable to our lifetime</em> (run across millions of processors!) to achieve some real-life optimization tasks.</p> <p>This would <em>forbid</em> us from iterating across the parameter space fast enough — <em>Game over</em>.</p> <p>Efficiency in classical methods <strong>won’t</strong> provide the scaling we need to overcome this barrier. <br> <strong>A new paradigm is needed.</strong></p> <p>Fortunately, a large class of algorithms can handle this <strong>curse of dimensionality</strong> exceptionally well. <em>Can you guess what we are talking about?</em> <br> Yup, <em>Deep Learning</em>!</p> <p>In the following posts of this series, we will address how we can leverage on those approaches to possibly overcome these issues raised by classical methods.</p> <p><em>Until then, don’t melt away!</em> å</p> <h2 id="references--remarks">References &amp; Remarks</h2> <p><a href="https://www.machinedesign.com/3d-printing-cad/fea-and-simulation/article/21832072/whats-the-difference-between-fem-fdm-and-fvm" rel="external nofollow noopener" target="_blank">[1]</a> More on this FEM and FDM possible similarity here. <br> <a href="https://levelup.gitconnected.com/solving-2d-heat-equation-numerically-using-python-3334004aa01a" rel="external nofollow noopener" target="_blank">[2]</a> The FDM-based approach we present is loosely following this excellent example that inspired us.<br> <a href="https://www.uni-muenster.de/imperia/md/content/physik_tp/lectures/ws2016-2017/num_methods_i/heat.pdf" rel="external nofollow noopener" target="_blank">[3]</a> More details of the derivation of this expression can be found here (pages 11/12). <br> <strong>[4]</strong> There are more complex cases where it is convenient to define such <em>staggered grids</em> with fractional offsets to the original (e.g for Maxwell’s Equations).</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Manuel Madeira. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["\\[","\\]"]],tags:"ams"},svg:{fontCache:"global"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-6BNW5R0VXG"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-6BNW5R0VXG");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>