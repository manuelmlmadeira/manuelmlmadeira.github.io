<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="-mRHvwnHdFYIa-KaKIgZIGLfsdeDLGZ9rZEsWECSiuE"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Heat #4 - Generalized Neuro-Solver for the Heat Equation | Manuel Madeira</title> <meta name="author" content="Manuel Madeira"> <meta name="description" content="Fourth and last blogpost of the Heat series. It showcases how to use Physics-Informed Neural Networks (PINNs) to solve partial differential equations (in particular, the Heat equation) for arbitrary initial/boundary conditions and domain geometries."> <meta name="keywords" content="Manuel, Madeira, personal, web, site, page, machine learning, artificial intelligence"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://manuelmlmadeira.github.io/blog/2022/heat-4-neurosolver/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Manuel¬†<span class="font-weight-bold">Madeira</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Heat #4 - Generalized Neuro-Solver for the Heat Equation</h1> <p class="post-meta">April 8, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </header> <article class="post-content"> <p><strong>Authors</strong>: Manuel Madeira, David Carvalho</p> <p><strong>Reviewers</strong>: F√°bio Cruz</p> <p><em>This blog post was developed while working at <a href="https://inductiva.ai/" rel="external nofollow noopener" target="_blank">Inductiva Research Labs</a>.</em></p> <hr> <p>In this section, we are going to show you that a grand unification of gravity within a quantum field theory framework can explain topologically nontrivial dynamics observed in simulations of graviton inelastic collisions in a AdS space. Let‚Äôs go!</p> <div style="text-align: center;"> <img src="../../../assets/img/blogposts/heat4/shaq.gif"> </div> <p>It‚Äôs all good. We‚Äôre just messing with your mental sanity!</p> <p><em>Ok</em>. Now that we have caught your attention, let us snap back to reality ‚Äî maybe one day we will pay this topic some attention!</p> <p>Even though we won‚Äôt achieve such an ambitious goal as the one mentioned above, in this final post of the <em>Heat series</em>, we are going to fulfill our overarching promise:</p> <blockquote> <p>To showcase the strength and versatility of Neural Networks (NNs) in solving hard Partial Differential Equations (PDEs) in challenging domains and conditions.</p> </blockquote> <p>In this final instalment of the <em>Heat series</em>, we delve once more into this topic with the aid of our very cherished <em>Heat Equation</em>. So let‚Äôs go!</p> <h2 id="introduction">Introduction</h2> <p>So far in this series, we showcased a classical algorithm and a Neural Network (NN) to solve the Heat equation. Both of these implementations were performed on an <strong>extremely simplistic (and unrealistic) scenario.</strong> <br> It is as straightforward as it can get:</p> <ul> <li> <strong>the heat equation is a <em>linear</em> PDE</strong>: the solution function and all its partial derivatives occur <em>linearly</em>, without any cross products or powers and any spacial preference for any of the (only) two coordinates. <em>This form of PDE is one of the simplest we can face.</em> </li> <li> <strong>a very idealized medium with a constant diffusivity $D$ was used</strong>: admittedly a huge simplification, as it assumes a perfectly homogeneous and stationary medium. In a more plausible setting, we would have to consider a function $D(t, x, y)$ instead.</li> <li> <strong>very simplistic conditions</strong>: the geometry considered of a square plate where the boundaries are <em>always</em> kept fixed and the heat flow uniformly set up along the upper boundary is quite utopian in practice. Real-life plates have deformities, changes in density, imperfections‚Ä¶</li> </ul> <p>So, we ought to ask the question:</p> <blockquote> <p>How can we strengthen our case ‚Äî that NNs can handle more complex and nontrivial cases?</p> </blockquote> <p>This is exactly what we want to address. We will ramp up the complexity of these scenarios and see how well NNs can <em>potentially</em> fare.</p> <p>This won‚Äôt be done with much sophistication ‚Äî the approach we adopt is very incremental and straightforward. Most importantly though, it serves the purpose of a simple demonstration of the principle of <strong>generalizing</strong> learning algorithms.</p> <h2 id="rethinking-the-nn-architecture">Rethinking the NN architecture</h2> <p>Ok. What do we mean by <em>generalization power</em>?</p> <p>Think of a PDE ‚Äî apart from the structure of the differential operators acting on the solution, there are many other variations that can impact its behavior: internal parameters (such as physical properties) but also the initial and boundary conditions (IBCs).</p> <p>Getting a NN to learn for a particular instantiation of these conditions is hard on its own. We could naively consider adding extra dimensions and run a NN that would be trained point by point by grid-searching on yet a bigger space.</p> <p>This surely does <strong>not</strong> sound scalable. If we want to see the effect obtained from ever-so-slightly different conditions on the same PDE, we have to rerun the <strong>whole</strong> classical method or perform the <strong>entire</strong> NN training task again. <br> <em>This is highly unsatisfactory!</em></p> <p>Worse still: the required time to train the PINN increases rather substantially. <em>Generalization power</em> requires <em>heavy</em> computational resources and routines that must handle the completion of the algorithm efficiently. In either case, the <strong>training time naturally becomes a major bottleneck</strong>.</p> <p>How to tackle these issues?</p> <h2 id="hard-encoding-the-variability">Hard-(en)coding the variability</h2> <p>There are two direct answers ‚Äî either we directly:</p> <ul> <li> <p><em>decrease</em> the training time by exploiting some magically-enhanced novel procedure which can accelerate the PINN‚Äôs weights and biases fitting <em>somehow</em>. As we discussed before, this seems quite unlikely and it is unclear if this factor could counteract the scaling effects.</p> </li> <li> <p><em>increase</em> the yield from that training process: what if, for the same effort in training, we could obtain better predictions or cover a larger set of physical cases?</p> </li> </ul> <p>It is from the second direction that the generalization idea comes out. Given that we will have to train our PINN anyway,</p> <blockquote> <p>why don‚Äôt we try to make it learn the solution function for different sets of internal or external factors <strong>from the get-go</strong>?</p> </blockquote> <p>We can do this by <strong>hard-coding</strong> the prescription of conditions or parameters by including appropriate representations in input space <strong>directly</strong> in the model. Only then we can say the model has a shot at <em>learning</em> for generalized cases.</p> <p>But this isn‚Äôt trivial by itself. Imagine you want to use a vanilla NN in a supervised way. To train it, you would need a ground truth given by say, a classical algorithm. Each such process would have to rerun for each new set of parameters or conditions. This can take a lot of time.</p> <p>On top of that, how to know which conditions to sample from? Depending on the quality of the sampling used to generate the ground truth (in this case given by a classical simulation algorithm), the model can now <em>in principle</em> be used as an <strong>oracle</strong> which, if well trained, will return confident outputs for <strong>unseen</strong> parameters. But we now know sampling the phase space can be extremely slow or downright unfeasible in this setting. So we may wonder:</p> <blockquote> <p>But what if we don‚Äôt need a ground truth at all in our NN?</p> </blockquote> <p>Well, we would <strong>bypass</strong> the need to run <strong>expensive and heavy</strong> classical algorithms!</p> <p>Granted, constructing efficient <strong>neuro-solvers</strong> is far from trivial. However, the upshot of such hard and laborious work to get the model to learn the <em>generalized task</em> can be <strong>huge</strong> ‚Äî a <em>considerable advantage</em> in favor of NNs, as their inference can be <strong>significantly faster</strong> than classical methods.</p> <p>If NNs succeed in this task, they can <strong>potentially</strong> solve PDEs in theoretically <strong>all</strong> possible conditions! In this perspective, NNs supersede ‚Äúgrid-like‚Äù algorithms where adding such parameters results in an <strong>exponential</strong> curse of dimensionality.</p> <p><em>It sounds powerful, right?</em></p> <p>Now, let‚Äôs get things on more concrete ground‚Ä¶ You know the drill: <em>it‚Äôs time to get hot in here!</em></p> <h2 id="a-neuro-solver-for-the-heat-equation">A neuro-solver for the Heat Equation</h2> <p>We have been advertising PINNs (Physics-Informed Neural Networks) for their flexibility as a DL framework, so you guessed it right ‚Äî we are going to use them to showcase how generalizational power can be harvested from picking appropriate algorithms that can handle such beasts as <em>dem‚Äô</em> mighty PDEs!</p> <p>To test these notions of <em>generalization</em>, we will consider our usual setup of heat diffusion across a 2D rectangular plate:</p> <div style="text-align: center;"> <img style="width:40%;" src="../../../assets/img/blogposts/heat4/IBCs_hot_edge.png"> </div> <p><strong>Fig. 1</strong>: The usual initial and boundary conditions (IBCs) we assume to solve the Heat Equation on the 2D plate. Credits: David Carvalho / Inductiva.</p> <p>It states that the temperature profile $u(t,x,y)$ must satisfy:</p> \[\left[ \frac{\partial}{\partial t} - D \left( \frac{\partial^2}{\partial x ^2} + \frac{\partial^2}{\partial y ^2} \right) \right]u(t,x,y)= 0,\] <p>With it, let‚Äôs investigate three topics:</p> <ul> <li> <strong>Learning for parametrized boundary conditions</strong>: keeping this admittedly simple domain, we <em>parametrize</em> the top edge temperature $u_{\rm top}$ into our PINN. After the network has been trained, we compare its prediction for an <em>unseen</em> top edge temperature by benchmarking it with respect to the classical algorithm (FDM) output.</li> <li> <strong>Learning for different domains</strong>: we see how well PINNs can solve when using more complex geometries. We will solve the Heat Equation with a PINN in a more challenging domain, where a spherical hole is punched into the interior of the plate.</li> <li> <strong>Learning without generalizing</strong>: we will benchmark how much slower it gets if generalization principles are neglected. In other words, we will adress generalization by brute force. Using our new holed plate, we will run PINNs that can solve across this harder domain <em>when trained (each at a time) for various diffusitivities $D$</em>.</li> </ul> <h3 id="lets-heat-run">Let‚Äôs Heat <code class="language-plaintext highlighter-rouge">run</code> </h3> <p>You don‚Äôt need to program anything ‚Äî you can find and run our code in our dedicated <code class="language-plaintext highlighter-rouge">github</code> repository <a href="https://github.com/inductiva/blog_code_snippets" rel="external nofollow noopener" target="_blank">here</a> and train your powerful PINNs!</p> <h2 id="getting-a-pinn-to-generalize-across-boundary-conditions">Getting a PINN to generalize across boundary conditions</h2> <p>Until now, only <strong>fixed</strong> scenarios for which the boundary and initial conditions were set <em>a priori</em> were used (like the ones just above). In this framework, the PINN is trained to fit <strong>exclusively</strong> with those conditions.</p> <p>This is naturally far from ideal. If we were to change the initial temperature of any edge <strong>by a teeny tiny bit</strong>, the model output for such a system would already be of <strong>dubious</strong> predictive power!</p> <p>So, we face a structural question here:</p> <blockquote> <p>How can we encode this boundary information as input to the PINN in a way the model can effectively generalize its effect on the output solution?</p> </blockquote> <p>To answer this, let‚Äôs focus on an extremely simple setup to showcase this training capability. We will keep <em>all</em> boundary and initial conditions fixed <em>except</em> for the temperature of the top edge, which can now <em>change</em>.</p> <p>Once again, we pick the simplest approach to achieve generalization: via <strong>parametrization</strong>. In this way, we think of encoding the variation by means of <em>variables</em> (or any other sensible descriptor) to allow the NN to extend the solution function to other IBCs <em>natively</em> in its architecture.</p> <p>In this simple configuration, a single <strong>parameter</strong> $u_{\rm top}$ will become an <strong>additional</strong> input.</p> <div style="text-align: center;"> <img style="width:80%;" src="../../../assets/img/blogposts/heat4/PINN_top_edge.png"> </div> <p><strong>Fig 2</strong>: Our PINN will now be able to learn the behavior of the solution as the hot edge temperature $u_{\rm top}$ is an input of the model. Credits: David Carvalho / Inductiva.</p> <p>To see how well the PINN fares, we:</p> <ul> <li>train it by sampling many instances of $u_{\rm top}$ within the range $[-1, 1]^\mathrm{o}C$.</li> <li>then infer for the unseen case $u_{\rm top} = 0^\mathrm{o}C$.</li> </ul> <h3 id="running">Running‚Ä¶</h3> <p>Do you remember when in we mentioned that our implementation was able to accommodate some extra complexity? Time to exploit it!</p> <p>The command line instruction to trigger this experiment and generate the PINN output is simply:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python heat_idrlnet.py --max_iter=10000 --output_folder=generalization_bc --hot_edge_temp_range=-1,1 --hot_edge_temp_to_plot=0 --output_num_x=500 --output_num_y=500 --colorbar_limits=-1.2,1.2
</code></pre></div></div> <p>The script <code class="language-plaintext highlighter-rouge">heat_idrlnet.py</code> trains a PINN in the setting described throughout the Heat series. The flags in this command line fulfill different purposes:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">max_iter</code> defines the total number of training epochs;</li> <li> <code class="language-plaintext highlighter-rouge">output_folder</code> determines the directory where the resulting files stemming from the PINN training procedure are stored;</li> <li> <code class="language-plaintext highlighter-rouge">hot_edge_temp_range</code> is the range of hot edge temperatures within which the PINN is trained;</li> <li> <code class="language-plaintext highlighter-rouge">hot_edge_temp_to_plot</code> is the hot edge temperature to which we intend to infer results;</li> <li> <code class="language-plaintext highlighter-rouge">output_num_x</code> and <code class="language-plaintext highlighter-rouge">output_num_y</code> define the discretization along the x-axis and y-axis, respectively, of the grid in which we infer results;</li> <li> <code class="language-plaintext highlighter-rouge">colorbar_limits</code> defines the range of the colorbar used.</li> </ul> <p>Let‚Äôs analyze it by using a classical Finite Difference Method (FDM) for $u_{\rm top} =0 \;^\mathrm{o}C$) as the benchmark.</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/generalization_bc.mp4" type="video/mp4"></source> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/generalization_bc_fdm.mp4" type="video/mp4"></source> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/generalization_bc_error.mp4" type="video/mp4"></source> </video> </div> <p><strong>Fig. 3</strong>: A PINN estimate of the solution of the Heat Equation for a top edge temperature $u_{\rm top} = 0^\mathrm{o}C$ [top], the output generated by a classical (FDM) method [middle] and their difference [bottom]. Credits: Manuel Madeira / Inductiva.</p> <p>Very nice! As expected, the network [top] recovers the diffusion patterns predicted by the classical algorithm [middle]. We can track the error by plotting their difference [bottom], where a great resemblance arises. This plot can be easily obtained by running the provided <code class="language-plaintext highlighter-rouge">heat_error.py</code> python script. We notice that the main source of error is found in the upper corners where cold and hot edges get in touch, generating an extremely sharp transition that the PINN struggles to keep up with.</p> <p>Even though some minor numerical deviations are seen, these are justifiable given that the task that we have provided to the PINN is <strong>significantly harder</strong>, and we kept the total number of epochs and Neural Network architecture as before in the series.</p> <p><em>Lesson:</em> for the same amount of training, clever architectures can indeed provide us the generalization power we sought, saving us a huge amount of computation resources and with very little damage in results accuracy!</p> <h2 id="probing-complex-geometries">Probing complex geometries</h2> <p>We are interested in testing PINNs for more complex geometries than the regular square plate. Let us then now go the extra mile and address precisely the challenges of probing <strong>different domains</strong> with NNs.</p> <p>PINNs are particularly well suited to address complex geometries as it only requires <strong>a proper domain sampler</strong> that provides both:</p> <ul> <li>boundary and initial points with the correct target value (given by the ground truth);</li> <li>and interior points where the PINN computes the PDE residual and then backpropagates it.</li> </ul> <p>In fact, the PINN solution function will be defined for values outside of the domain considered, but we just neglect it.</p> <p>Our code implementation supports designing a plate with an arbitrary number of holes inside the problem domain. Let‚Äôs focus on a single hole at the plate center:</p> <div style="text-align: center;"> <img style="width:60%;" src="../../../assets/img/blogposts/heat4/IBCs_hole.png"> </div> <p><strong>Fig. 4</strong>: We now generalize our boundary and initial conditions given the domain by taking the top edge temperature as a variable parameter $u_{\rm top} \in [-1,1] \;^\mathrm{o}C$, while the hole boundary is of the hot or cold type. Credits: David Carvalho / Inductiva.</p> <p>Given this, we keep the boundary and initial conditions as in the previous setting: top edge at the maximal temperature ($u = 1\;^\mathrm{o}C$) and the rest of the boundaries and initial points at the minimal temperature ($u = -1\;^\mathrm{o}C$).</p> <p>We consider two types of holes now:</p> <ul> <li> <strong>Hot hole</strong>: The points sampled from the hole boundary are set to the maximal temperature ($u = 1\;^\mathrm{o}C$);</li> <li> <strong>Cold hole</strong>: Conversely, in this case, the points sampled from the hole boundary are set to the minimal temperature ($u = -1\;^\mathrm{o}C$).</li> </ul> <h2 id="running-our-code">Running our code</h2> <p>Let‚Äôs now get some code running! The instruction in the command line that leads to the PINN results is the following:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python heat_idrlnet.py --max_iter=10000 --output_folder=hot_hole --holes_list=0,0,0.1 --output_num_x=500 --output_num_y=500 --colorbar_limits=-1.5,1.5
</code></pre></div></div> <p>For instance, the cold hole setting can be run as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python heat_idrlnet.py --max_iter=10000 --output_folder=cold_hole --holes_list=0,0,0.1 --holes_temp=-1 --output_num_x=500 --output_num_y=500 --colorbar_limits=-1.5,1.5
</code></pre></div></div> <p>Regarding the new flags in these command lines:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">holes_list</code> is the list of holes we consider in our plate, where each group of three contiguous entries define a hole as $(x_\rm{center}, y_\rm{center}, radius)$ ;</li> <li> <code class="language-plaintext highlighter-rouge">holes_temp</code> defines the temperature of the holes boundaries (it is not defined for the hot hole as it is $1\;^\mathrm{o}C$ by default);</li> </ul> <p>So, for the same $u_{\rm top} = 1\;^\mathrm{o}C$, we see the difference in the profile for both the cold and hot hole edge scenarios:</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/hot_hole.mp4" type="video/mp4"></source> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/cold_hole.mp4" type="video/mp4"></source> </video> </div> <p><strong>Fig. 5</strong>: Heat diffusion profiles for a hot [top] and cold [bottom] temperature of the hole boundary. Case on point: small changes in parameters can result in very different outputs! Credits: Manuel Madeira / Inductiva.</p> <p><em>Woah</em>! The results from these experiments are clear: the hole in the domain clearly affects the heat diffusion profile in <strong>very different</strong> outputs!</p> <ul> <li>When the hole boundary is cold, the heat source remains the same and so the same general downward parabolic-like behavior we‚Äôve discussed is observed. The main difference is that heat flows <em>around</em> the hole.</li> <li>The more interesting case occurs when we also pump energy into the plate through the hole boundary. In that case, another pattern is added ‚Äì a radial outflow. The interference between these two streams is obtained in opposite directions: while the cold hole suppresses the heat diffusion towards closer regions to the hole border.</li> </ul> <p>But this is still intuitive enough. But how to think of a highly complex interference pattern? Let‚Äôs put our code handling more exotic domains!</p> <p>For instance, let‚Äôs think of more physically-relevant cases. Can we understand the physics behind this irregular setting where 3 holes of various sizes and positions are found and the boundary is now curved?</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/3_holes.mp4" type="video/mp4"></source> </video> </div> <p><strong>Fig. 6</strong>: Heat flow across a more complex domain composed of three holes of varying sizes and positions, as well as curved left and right boundaries. Credits: Manuel Madeira / Inductiva.</p> <h2 id="generalizing-through-grid-searching-is-inefficient">Generalizing through grid-searching (is inefficient)</h2> <p>To make a point (and get more awesome visualizations üòÅ), let‚Äôs see how the output changes by changing the diffusitivity rate $D$ for the hot hole scenario.</p> <p>For that, we simply run each PDE for each $D$:</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/hot_hole_d0.01.mp4" type="video/mp4"></source> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/hot_hole.mp4" type="video/mp4"></source> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/hot_hole_d1.mp4" type="video/mp4"></source> </video> </div> <p><strong>Fig. 7</strong>: Heat diffusion profile in the holed domain by ramping up the diffusitivity from $D=0.01$ [top] to $D=0.1$ [middle] and $D=1$ [bottom]. Note that each PINN had to be trained individually. Credits: Manuel Madeira / Inductiva.</p> <p><em>Hot</em>! We can see that the diffusitivity allows us to disentangle both phenomena streams we discussed (downward vs radial). Additionally, it sets the time scale for <em>equilibration</em>, when the state becomes <em>stationary</em> <em>i.e.</em> whenever $\frac{\partial u}{\partial t} = 0 $</p> <p>The main point to grab here though is that the output you see comes after training each PINN <strong>individually</strong>! For instance, for these settings, each PINN will take <strong>about 5 hours</strong>. This is completely <strong>inefficient</strong> and does not allow the algorithm to organically understand how to map the IBCs to the output.</p> <h3 id="the-pinn-architecture-behind-generalization">The PINN architecture behind generalization</h3> <p>In order to make generalization tangible, the computing infrastructure needs to be versatile and efficient. From a computational perspective, we should ask:</p> <blockquote> <p>How exactly does the PINN change to accommodate for this generalization?</p> </blockquote> <p>The fact that the loss function differs according to the domain from where the data domain points considered are sampled is of <strong>huge computational relevance</strong>.</p> <p>While a initial or boundary point is directly fit by the PINN to its target (imposed by the initial or boundary condition itself), a point stemming from an interior domain contributes to the fitting procedure through its PDE residue.</p> <p>PINNs do not impose an upper limit to the number of IBCs or interior domains. Each of these IBCs may have a different target and each interior domain might be assigned to different PDEs. As you can imagine, for complex problems, PINNs have a high chance of turning into a mess!</p> <p>Let‚Äôs make use of a perk from the IDRLnet library (which supported us throughout the Heat series) ‚Äî a visual representation of the computational graph (in terms of <code class="language-plaintext highlighter-rouge">IDRLnet nodes</code>) underneath the implementation it receives.</p> <p>For this our instance, the representations obtained can be visualized as:</p> <div style="text-align: center;"> <img style="width:80%;" src="../../../assets/img/blogposts/heat4/nodes.png"> </div> <p><strong>Fig. 8</strong>: Computational graphs considered by IDRLnet for each sampling domain considered. If we added holes to our plate, an extra graph would be obtained (similar to the ones from the IBCs). Credits: Manuel Madeira / Inductiva.</p> <p>Note that the blue nodes are obtained by sampling the different domains considered (DataNodes), the red nodes are computational (PDENodes or NetNodes), and the green nodes are constraints (targets). [2]</p> <p>Having a graphical representation of what is going on inside our code is always helpful. Naturally, this tool may become extremely handy to ensure that the problem solution is well implemented.</p> <h2 id="conclusion">Conclusion</h2> <p>Well, it has been quite a ride! To finish off the tutorial, we took the opportunity to sail through seas that classical methods can not achieve (or at least through simple procedures). The reason is that they are not scalable with increasing parameter customization. Classical methods have underwhelming potential in providing acceleration into solving PDEs.</p> <p>We argue Neural Networks have the capability of streamlining these high-dimensional increments by generalizing more smartly. But the complexity has been transferred to other issues. As we alluded before, the choice and fine-tuning of variables and parameters is not something trivial to achieve (either through classical and DL frameworks).</p> <p>To see how versatile NNs can be, we pushed a little harder and checked if adding more complexity could be coped by Physics-Informed Neural Networks (PINNs). In a modest setup of incremental difficulty, we sure have explored a lot of situations and probed potential means to achieve smarter training.</p> <p>This strategy of adding new parameters as simple inputs to achieve generalization is the simplest one (but arguably not the most efficient one).</p> <p>There are several alternatives ‚Äî a great example is <em>HyperPINNs</em> [1], whose results have been published recently. They result from the fusion of hypernetworks and the more conventional PINNs. Most importantly, HyperPINNs have been shown to succeed in achieving generalization ‚Äî although in a different implementational flavor.</p> <p>The outlook message to us is simple to state. The issues pointed out and run experiments illustrate two essential aspects that will need to continue being optimized are different in nature:</p> <ul> <li>the power that Machine/Deep Learning techniques have to propel scientific computing to unseen new paradigms;</li> <li>the challenges in computation and algorithm architecture caused by evermore realistic and refined systems of interest.</li> </ul> <h2 id="references--remarks">References &amp; Remarks</h2> <p><a href="https://arxiv.org/abs/2111.01008" rel="external nofollow noopener" target="_blank">[1]</a> A great introduction to HyperPINNs!</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2025 Manuel Madeira. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["\\[","\\]"]],tags:"ams"},svg:{fontCache:"global"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-6BNW5R0VXG"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-6BNW5R0VXG");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>