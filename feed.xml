<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://manuelmlmadeira.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://manuelmlmadeira.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-11T00:41:32+01:00</updated><id>https://manuelmlmadeira.github.io/feed.xml</id><title type="html">blank</title><subtitle>Manuel Madeira&apos;s personal website. </subtitle><entry><title type="html">DeFoG: Discrete Flow Matching for Graph Generation</title><link href="https://manuelmlmadeira.github.io/blog/2025/defog/" rel="alternate" type="text/html" title="DeFoG: Discrete Flow Matching for Graph Generation"/><published>2025-11-09T01:00:00+01:00</published><updated>2025-11-09T01:00:00+01:00</updated><id>https://manuelmlmadeira.github.io/blog/2025/defog</id><content type="html" xml:base="https://manuelmlmadeira.github.io/blog/2025/defog/"><![CDATA[ <p>This blogpost centralizes all materials, presentations, and outreach related to the paper <strong>DeFoG: Discrete Flow Matching for Graph Generation</strong>.</p> <h4 id="-main-resources">üìò Main Resources</h4> <ul> <li><a href="https://arxiv.org/abs/2410.04263">Paper (arXiv)</a></li> <li><a href="https://qym7.github.io/DeFoG-Web/">Project Page</a></li> <li><a href="https://github.com/manuelmlmadeira/DeFoG">Code Repository</a></li> </ul> <h4 id="-presentations">üé§ Presentations</h4> <ul> <li><a href="https://icml.cc/virtual/2025/poster/45644">ICML Short Presentation</a> (~3 min)</li> <li><a href="https://icml.cc/virtual/2025/oral/47238">ICML Oral Presentation</a> (~12 min)</li> <li><a href="https://www.youtube.com/live/cwZlshqSUHg?si=sq69iw0CaDzBRPNs">AI Alliance: AI in Materials &amp; Chemistry Webinar Series</a> (~50 min)</li> </ul> <h4 id="-outreach">üì∞ Outreach</h4> <ul> <li><a href="https://aihub.org/2025/09/17/discrete-flow-matching-framework-for-graph-generation/">AIhub Blog Post</a></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[This post gathers all materials, presentations, and outreach related to DeFoG, a discrete flow matching framework for graph generation.]]></summary></entry><entry><title type="html">Heat #4 - Generalized Neuro-Solver for the Heat Equation</title><link href="https://manuelmlmadeira.github.io/blog/2022/heat-4-neurosolver/" rel="alternate" type="text/html" title="Heat #4 - Generalized Neuro-Solver for the Heat Equation"/><published>2022-04-08T02:00:00+02:00</published><updated>2022-04-08T02:00:00+02:00</updated><id>https://manuelmlmadeira.github.io/blog/2022/heat-4-neurosolver</id><content type="html" xml:base="https://manuelmlmadeira.github.io/blog/2022/heat-4-neurosolver/"><![CDATA[ <p><strong>Authors</strong>: Manuel Madeira, David Carvalho</p> <p><strong>Reviewers</strong>: F√°bio Cruz</p> <p><em>This blog post was developed while working at <a href="https://inductiva.ai/">Inductiva Research Labs</a>.</em></p> <hr/> <p>In this section, we are going to show you that a grand unification of gravity within a quantum field theory framework can explain topologically nontrivial dynamics observed in simulations of graviton inelastic collisions in a AdS space. Let‚Äôs go!</p> <div style="text-align: center;"> <img src="../../../assets/img/blogposts/heat4/shaq.gif"/> </div> <p>It‚Äôs all good. We‚Äôre just messing with your mental sanity!</p> <p><em>Ok</em>. Now that we have caught your attention, let us snap back to reality ‚Äî maybe one day we will pay this topic some attention!</p> <p>Even though we won‚Äôt achieve such an ambitious goal as the one mentioned above, in this final post of the <em>Heat series</em>, we are going to fulfill our overarching promise:</p> <blockquote> <p>To showcase the strength and versatility of Neural Networks (NNs) in solving hard Partial Differential Equations (PDEs) in challenging domains and conditions.</p> </blockquote> <p>In this final instalment of the <em>Heat series</em>, we delve once more into this topic with the aid of our very cherished <em>Heat Equation</em>. So let‚Äôs go!</p> <h2 id="introduction">Introduction</h2> <p>So far in this series, we showcased a classical algorithm and a Neural Network (NN) to solve the Heat equation. Both of these implementations were performed on an <strong>extremely simplistic (and unrealistic) scenario.</strong> <br/> It is as straightforward as it can get:</p> <ul> <li><strong>the heat equation is a <em>linear</em> PDE</strong>: the solution function and all its partial derivatives occur <em>linearly</em>, without any cross products or powers and any spacial preference for any of the (only) two coordinates. <em>This form of PDE is one of the simplest we can face.</em></li> <li><strong>a very idealized medium with a constant diffusivity $D$ was used</strong>: admittedly a huge simplification, as it assumes a perfectly homogeneous and stationary medium. In a more plausible setting, we would have to consider a function $D(t, x, y)$ instead.</li> <li><strong>very simplistic conditions</strong>: the geometry considered of a square plate where the boundaries are <em>always</em> kept fixed and the heat flow uniformly set up along the upper boundary is quite utopian in practice. Real-life plates have deformities, changes in density, imperfections‚Ä¶</li> </ul> <p>So, we ought to ask the question:</p> <blockquote> <p>How can we strengthen our case ‚Äî that NNs can handle more complex and nontrivial cases?</p> </blockquote> <p>This is exactly what we want to address. We will ramp up the complexity of these scenarios and see how well NNs can <em>potentially</em> fare.</p> <p>This won‚Äôt be done with much sophistication ‚Äî the approach we adopt is very incremental and straightforward. Most importantly though, it serves the purpose of a simple demonstration of the principle of <strong>generalizing</strong> learning algorithms.</p> <h2 id="rethinking-the-nn-architecture">Rethinking the NN architecture</h2> <p>Ok. What do we mean by <em>generalization power</em>?</p> <p>Think of a PDE ‚Äî apart from the structure of the differential operators acting on the solution, there are many other variations that can impact its behavior: internal parameters (such as physical properties) but also the initial and boundary conditions (IBCs).</p> <p>Getting a NN to learn for a particular instantiation of these conditions is hard on its own. We could naively consider adding extra dimensions and run a NN that would be trained point by point by grid-searching on yet a bigger space.</p> <p>This surely does <strong>not</strong> sound scalable. If we want to see the effect obtained from ever-so-slightly different conditions on the same PDE, we have to rerun the <strong>whole</strong> classical method or perform the <strong>entire</strong> NN training task again. <br/> <em>This is highly unsatisfactory!</em></p> <p>Worse still: the required time to train the PINN increases rather substantially. <em>Generalization power</em> requires <em>heavy</em> computational resources and routines that must handle the completion of the algorithm efficiently. In either case, the <strong>training time naturally becomes a major bottleneck</strong>.</p> <p>How to tackle these issues?</p> <h2 id="hard-encoding-the-variability">Hard-(en)coding the variability</h2> <p>There are two direct answers ‚Äî either we directly:</p> <ul> <li> <p><em>decrease</em> the training time by exploiting some magically-enhanced novel procedure which can accelerate the PINN‚Äôs weights and biases fitting <em>somehow</em>. As we discussed before, this seems quite unlikely and it is unclear if this factor could counteract the scaling effects.</p> </li> <li> <p><em>increase</em> the yield from that training process: what if, for the same effort in training, we could obtain better predictions or cover a larger set of physical cases?</p> </li> </ul> <p>It is from the second direction that the generalization idea comes out. Given that we will have to train our PINN anyway,</p> <blockquote> <p>why don‚Äôt we try to make it learn the solution function for different sets of internal or external factors <strong>from the get-go</strong>?</p> </blockquote> <p>We can do this by <strong>hard-coding</strong> the prescription of conditions or parameters by including appropriate representations in input space <strong>directly</strong> in the model. Only then we can say the model has a shot at <em>learning</em> for generalized cases.</p> <p>But this isn‚Äôt trivial by itself. Imagine you want to use a vanilla NN in a supervised way. To train it, you would need a ground truth given by say, a classical algorithm. Each such process would have to rerun for each new set of parameters or conditions. This can take a lot of time.</p> <p>On top of that, how to know which conditions to sample from? Depending on the quality of the sampling used to generate the ground truth (in this case given by a classical simulation algorithm), the model can now <em>in principle</em> be used as an <strong>oracle</strong> which, if well trained, will return confident outputs for <strong>unseen</strong> parameters. But we now know sampling the phase space can be extremely slow or downright unfeasible in this setting. So we may wonder:</p> <blockquote> <p>But what if we don‚Äôt need a ground truth at all in our NN?</p> </blockquote> <p>Well, we would <strong>bypass</strong> the need to run <strong>expensive and heavy</strong> classical algorithms!</p> <p>Granted, constructing efficient <strong>neuro-solvers</strong> is far from trivial. However, the upshot of such hard and laborious work to get the model to learn the <em>generalized task</em> can be <strong>huge</strong> ‚Äî a <em>considerable advantage</em> in favor of NNs, as their inference can be <strong>significantly faster</strong> than classical methods.</p> <p>If NNs succeed in this task, they can <strong>potentially</strong> solve PDEs in theoretically <strong>all</strong> possible conditions! In this perspective, NNs supersede ‚Äúgrid-like‚Äù algorithms where adding such parameters results in an <strong>exponential</strong> curse of dimensionality.</p> <p><em>It sounds powerful, right?</em></p> <p>Now, let‚Äôs get things on more concrete ground‚Ä¶ You know the drill: <em>it‚Äôs time to get hot in here!</em></p> <h2 id="a-neuro-solver-for-the-heat-equation">A neuro-solver for the Heat Equation</h2> <p>We have been advertising PINNs (Physics-Informed Neural Networks) for their flexibility as a DL framework, so you guessed it right ‚Äî we are going to use them to showcase how generalizational power can be harvested from picking appropriate algorithms that can handle such beasts as <em>dem‚Äô</em> mighty PDEs!</p> <p>To test these notions of <em>generalization</em>, we will consider our usual setup of heat diffusion across a 2D rectangular plate:</p> <div style="text-align: center;"> <img style="width:40%;" src="../../../assets/img/blogposts/heat4/IBCs_hot_edge.png"/> </div> <p><strong>Fig. 1</strong>: The usual initial and boundary conditions (IBCs) we assume to solve the Heat Equation on the 2D plate. Credits: David Carvalho / Inductiva.</p> <p>It states that the temperature profile $u(t,x,y)$ must satisfy:</p> \[\left[ \frac{\partial}{\partial t} - D \left( \frac{\partial^2}{\partial x ^2} + \frac{\partial^2}{\partial y ^2} \right) \right]u(t,x,y)= 0,\] <p>With it, let‚Äôs investigate three topics:</p> <ul> <li><strong>Learning for parametrized boundary conditions</strong>: keeping this admittedly simple domain, we <em>parametrize</em> the top edge temperature $u_{\rm top}$ into our PINN. After the network has been trained, we compare its prediction for an <em>unseen</em> top edge temperature by benchmarking it with respect to the classical algorithm (FDM) output.</li> <li><strong>Learning for different domains</strong>: we see how well PINNs can solve when using more complex geometries. We will solve the Heat Equation with a PINN in a more challenging domain, where a spherical hole is punched into the interior of the plate.</li> <li><strong>Learning without generalizing</strong>: we will benchmark how much slower it gets if generalization principles are neglected. In other words, we will adress generalization by brute force. Using our new holed plate, we will run PINNs that can solve across this harder domain <em>when trained (each at a time) for various diffusitivities $D$</em>.</li> </ul> <h3 id="lets-heat-run">Let‚Äôs Heat <code class="language-plaintext highlighter-rouge">run</code></h3> <p>You don‚Äôt need to program anything ‚Äî you can find and run our code in our dedicated <code class="language-plaintext highlighter-rouge">github</code> repository <a href="https://github.com/inductiva/blog_code_snippets">here</a> and train your powerful PINNs!</p> <h2 id="getting-a-pinn-to-generalize-across-boundary-conditions">Getting a PINN to generalize across boundary conditions</h2> <p>Until now, only <strong>fixed</strong> scenarios for which the boundary and initial conditions were set <em>a priori</em> were used (like the ones just above). In this framework, the PINN is trained to fit <strong>exclusively</strong> with those conditions.</p> <p>This is naturally far from ideal. If we were to change the initial temperature of any edge <strong>by a teeny tiny bit</strong>, the model output for such a system would already be of <strong>dubious</strong> predictive power!</p> <p>So, we face a structural question here:</p> <blockquote> <p>How can we encode this boundary information as input to the PINN in a way the model can effectively generalize its effect on the output solution?</p> </blockquote> <p>To answer this, let‚Äôs focus on an extremely simple setup to showcase this training capability. We will keep <em>all</em> boundary and initial conditions fixed <em>except</em> for the temperature of the top edge, which can now <em>change</em>.</p> <p>Once again, we pick the simplest approach to achieve generalization: via <strong>parametrization</strong>. In this way, we think of encoding the variation by means of <em>variables</em> (or any other sensible descriptor) to allow the NN to extend the solution function to other IBCs <em>natively</em> in its architecture.</p> <p>In this simple configuration, a single <strong>parameter</strong> $u_{\rm top}$ will become an <strong>additional</strong> input.</p> <div style="text-align: center;"> <img style="width:80%;" src="../../../assets/img/blogposts/heat4/PINN_top_edge.png"/> </div> <p><strong>Fig 2</strong>: Our PINN will now be able to learn the behavior of the solution as the hot edge temperature $u_{\rm top}$ is an input of the model. Credits: David Carvalho / Inductiva.</p> <p>To see how well the PINN fares, we:</p> <ul> <li>train it by sampling many instances of $u_{\rm top}$ within the range $[-1, 1]^\mathrm{o}C$.</li> <li>then infer for the unseen case $u_{\rm top} = 0^\mathrm{o}C$.</li> </ul> <h3 id="running">Running‚Ä¶</h3> <p>Do you remember when in we mentioned that our implementation was able to accommodate some extra complexity? Time to exploit it!</p> <p>The command line instruction to trigger this experiment and generate the PINN output is simply:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python heat_idrlnet.py --max_iter=10000 --output_folder=generalization_bc --hot_edge_temp_range=-1,1 --hot_edge_temp_to_plot=0 --output_num_x=500 --output_num_y=500 --colorbar_limits=-1.2,1.2
</code></pre></div></div> <p>The script <code class="language-plaintext highlighter-rouge">heat_idrlnet.py</code> trains a PINN in the setting described throughout the Heat series. The flags in this command line fulfill different purposes:</p> <ul> <li><code class="language-plaintext highlighter-rouge">max_iter</code> defines the total number of training epochs;</li> <li><code class="language-plaintext highlighter-rouge">output_folder</code> determines the directory where the resulting files stemming from the PINN training procedure are stored;</li> <li><code class="language-plaintext highlighter-rouge">hot_edge_temp_range</code> is the range of hot edge temperatures within which the PINN is trained;</li> <li><code class="language-plaintext highlighter-rouge">hot_edge_temp_to_plot</code> is the hot edge temperature to which we intend to infer results;</li> <li><code class="language-plaintext highlighter-rouge">output_num_x</code> and <code class="language-plaintext highlighter-rouge">output_num_y</code> define the discretization along the x-axis and y-axis, respectively, of the grid in which we infer results;</li> <li><code class="language-plaintext highlighter-rouge">colorbar_limits</code> defines the range of the colorbar used.</li> </ul> <p>Let‚Äôs analyze it by using a classical Finite Difference Method (FDM) for $u_{\rm top} =0 \;^\mathrm{o}C$) as the benchmark.</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/generalization_bc.mp4" type="video/mp4"/> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/generalization_bc_fdm.mp4" type="video/mp4"/> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/generalization_bc_error.mp4" type="video/mp4"/> </video> </div> <p><strong>Fig. 3</strong>: A PINN estimate of the solution of the Heat Equation for a top edge temperature $u_{\rm top} = 0^\mathrm{o}C$ [top], the output generated by a classical (FDM) method [middle] and their difference [bottom]. Credits: Manuel Madeira / Inductiva.</p> <p>Very nice! As expected, the network [top] recovers the diffusion patterns predicted by the classical algorithm [middle]. We can track the error by plotting their difference [bottom], where a great resemblance arises. This plot can be easily obtained by running the provided <code class="language-plaintext highlighter-rouge">heat_error.py</code> python script. We notice that the main source of error is found in the upper corners where cold and hot edges get in touch, generating an extremely sharp transition that the PINN struggles to keep up with.</p> <p>Even though some minor numerical deviations are seen, these are justifiable given that the task that we have provided to the PINN is <strong>significantly harder</strong>, and we kept the total number of epochs and Neural Network architecture as before in the series.</p> <p><em>Lesson:</em> for the same amount of training, clever architectures can indeed provide us the generalization power we sought, saving us a huge amount of computation resources and with very little damage in results accuracy!</p> <h2 id="probing-complex-geometries">Probing complex geometries</h2> <p>We are interested in testing PINNs for more complex geometries than the regular square plate. Let us then now go the extra mile and address precisely the challenges of probing <strong>different domains</strong> with NNs.</p> <p>PINNs are particularly well suited to address complex geometries as it only requires <strong>a proper domain sampler</strong> that provides both:</p> <ul> <li>boundary and initial points with the correct target value (given by the ground truth);</li> <li>and interior points where the PINN computes the PDE residual and then backpropagates it.</li> </ul> <p>In fact, the PINN solution function will be defined for values outside of the domain considered, but we just neglect it.</p> <p>Our code implementation supports designing a plate with an arbitrary number of holes inside the problem domain. Let‚Äôs focus on a single hole at the plate center:</p> <div style="text-align: center;"> <img style="width:60%;" src="../../../assets/img/blogposts/heat4/IBCs_hole.png"/> </div> <p><strong>Fig. 4</strong>: We now generalize our boundary and initial conditions given the domain by taking the top edge temperature as a variable parameter $u_{\rm top} \in [-1,1] \;^\mathrm{o}C$, while the hole boundary is of the hot or cold type. Credits: David Carvalho / Inductiva.</p> <p>Given this, we keep the boundary and initial conditions as in the previous setting: top edge at the maximal temperature ($u = 1\;^\mathrm{o}C$) and the rest of the boundaries and initial points at the minimal temperature ($u = -1\;^\mathrm{o}C$).</p> <p>We consider two types of holes now:</p> <ul> <li><strong>Hot hole</strong>: The points sampled from the hole boundary are set to the maximal temperature ($u = 1\;^\mathrm{o}C$);</li> <li><strong>Cold hole</strong>: Conversely, in this case, the points sampled from the hole boundary are set to the minimal temperature ($u = -1\;^\mathrm{o}C$).</li> </ul> <h2 id="running-our-code">Running our code</h2> <p>Let‚Äôs now get some code running! The instruction in the command line that leads to the PINN results is the following:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python heat_idrlnet.py --max_iter=10000 --output_folder=hot_hole --holes_list=0,0,0.1 --output_num_x=500 --output_num_y=500 --colorbar_limits=-1.5,1.5
</code></pre></div></div> <p>For instance, the cold hole setting can be run as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python heat_idrlnet.py --max_iter=10000 --output_folder=cold_hole --holes_list=0,0,0.1 --holes_temp=-1 --output_num_x=500 --output_num_y=500 --colorbar_limits=-1.5,1.5
</code></pre></div></div> <p>Regarding the new flags in these command lines:</p> <ul> <li><code class="language-plaintext highlighter-rouge">holes_list</code> is the list of holes we consider in our plate, where each group of three contiguous entries define a hole as $(x_\rm{center}, y_\rm{center}, radius)$ ;</li> <li><code class="language-plaintext highlighter-rouge">holes_temp</code> defines the temperature of the holes boundaries (it is not defined for the hot hole as it is $1\;^\mathrm{o}C$ by default);</li> </ul> <p>So, for the same $u_{\rm top} = 1\;^\mathrm{o}C$, we see the difference in the profile for both the cold and hot hole edge scenarios:</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/hot_hole.mp4" type="video/mp4"/> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/cold_hole.mp4" type="video/mp4"/> </video> </div> <p><strong>Fig. 5</strong>: Heat diffusion profiles for a hot [top] and cold [bottom] temperature of the hole boundary. Case on point: small changes in parameters can result in very different outputs! Credits: Manuel Madeira / Inductiva.</p> <p><em>Woah</em>! The results from these experiments are clear: the hole in the domain clearly affects the heat diffusion profile in <strong>very different</strong> outputs!</p> <ul> <li>When the hole boundary is cold, the heat source remains the same and so the same general downward parabolic-like behavior we‚Äôve discussed is observed. The main difference is that heat flows <em>around</em> the hole.</li> <li>The more interesting case occurs when we also pump energy into the plate through the hole boundary. In that case, another pattern is added ‚Äì a radial outflow. The interference between these two streams is obtained in opposite directions: while the cold hole suppresses the heat diffusion towards closer regions to the hole border.</li> </ul> <p>But this is still intuitive enough. But how to think of a highly complex interference pattern? Let‚Äôs put our code handling more exotic domains!</p> <p>For instance, let‚Äôs think of more physically-relevant cases. Can we understand the physics behind this irregular setting where 3 holes of various sizes and positions are found and the boundary is now curved?</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/3_holes.mp4" type="video/mp4"/> </video> </div> <p><strong>Fig. 6</strong>: Heat flow across a more complex domain composed of three holes of varying sizes and positions, as well as curved left and right boundaries. Credits: Manuel Madeira / Inductiva.</p> <h2 id="generalizing-through-grid-searching-is-inefficient">Generalizing through grid-searching (is inefficient)</h2> <p>To make a point (and get more awesome visualizations üòÅ), let‚Äôs see how the output changes by changing the diffusitivity rate $D$ for the hot hole scenario.</p> <p>For that, we simply run each PDE for each $D$:</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/hot_hole_d0.01.mp4" type="video/mp4"/> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/hot_hole.mp4" type="video/mp4"/> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat4/hot_hole_d1.mp4" type="video/mp4"/> </video> </div> <p><strong>Fig. 7</strong>: Heat diffusion profile in the holed domain by ramping up the diffusitivity from $D=0.01$ [top] to $D=0.1$ [middle] and $D=1$ [bottom]. Note that each PINN had to be trained individually. Credits: Manuel Madeira / Inductiva.</p> <p><em>Hot</em>! We can see that the diffusitivity allows us to disentangle both phenomena streams we discussed (downward vs radial). Additionally, it sets the time scale for <em>equilibration</em>, when the state becomes <em>stationary</em> <em>i.e.</em> whenever $\frac{\partial u}{\partial t} = 0 $</p> <p>The main point to grab here though is that the output you see comes after training each PINN <strong>individually</strong>! For instance, for these settings, each PINN will take <strong>about 5 hours</strong>. This is completely <strong>inefficient</strong> and does not allow the algorithm to organically understand how to map the IBCs to the output.</p> <h3 id="the-pinn-architecture-behind-generalization">The PINN architecture behind generalization</h3> <p>In order to make generalization tangible, the computing infrastructure needs to be versatile and efficient. From a computational perspective, we should ask:</p> <blockquote> <p>How exactly does the PINN change to accommodate for this generalization?</p> </blockquote> <p>The fact that the loss function differs according to the domain from where the data domain points considered are sampled is of <strong>huge computational relevance</strong>.</p> <p>While a initial or boundary point is directly fit by the PINN to its target (imposed by the initial or boundary condition itself), a point stemming from an interior domain contributes to the fitting procedure through its PDE residue.</p> <p>PINNs do not impose an upper limit to the number of IBCs or interior domains. Each of these IBCs may have a different target and each interior domain might be assigned to different PDEs. As you can imagine, for complex problems, PINNs have a high chance of turning into a mess!</p> <p>Let‚Äôs make use of a perk from the IDRLnet library (which supported us throughout the Heat series) ‚Äî a visual representation of the computational graph (in terms of <code class="language-plaintext highlighter-rouge">IDRLnet nodes</code>) underneath the implementation it receives.</p> <p>For this our instance, the representations obtained can be visualized as:</p> <div style="text-align: center;"> <img style="width:80%;" src="../../../assets/img/blogposts/heat4/nodes.png"/> </div> <p><strong>Fig. 8</strong>: Computational graphs considered by IDRLnet for each sampling domain considered. If we added holes to our plate, an extra graph would be obtained (similar to the ones from the IBCs). Credits: Manuel Madeira / Inductiva.</p> <p>Note that the blue nodes are obtained by sampling the different domains considered (DataNodes), the red nodes are computational (PDENodes or NetNodes), and the green nodes are constraints (targets). [2]</p> <p>Having a graphical representation of what is going on inside our code is always helpful. Naturally, this tool may become extremely handy to ensure that the problem solution is well implemented.</p> <h2 id="conclusion">Conclusion</h2> <p>Well, it has been quite a ride! To finish off the tutorial, we took the opportunity to sail through seas that classical methods can not achieve (or at least through simple procedures). The reason is that they are not scalable with increasing parameter customization. Classical methods have underwhelming potential in providing acceleration into solving PDEs.</p> <p>We argue Neural Networks have the capability of streamlining these high-dimensional increments by generalizing more smartly. But the complexity has been transferred to other issues. As we alluded before, the choice and fine-tuning of variables and parameters is not something trivial to achieve (either through classical and DL frameworks).</p> <p>To see how versatile NNs can be, we pushed a little harder and checked if adding more complexity could be coped by Physics-Informed Neural Networks (PINNs). In a modest setup of incremental difficulty, we sure have explored a lot of situations and probed potential means to achieve smarter training.</p> <p>This strategy of adding new parameters as simple inputs to achieve generalization is the simplest one (but arguably not the most efficient one).</p> <p>There are several alternatives ‚Äî a great example is <em>HyperPINNs</em> [1], whose results have been published recently. They result from the fusion of hypernetworks and the more conventional PINNs. Most importantly, HyperPINNs have been shown to succeed in achieving generalization ‚Äî although in a different implementational flavor.</p> <p>The outlook message to us is simple to state. The issues pointed out and run experiments illustrate two essential aspects that will need to continue being optimized are different in nature:</p> <ul> <li>the power that Machine/Deep Learning techniques have to propel scientific computing to unseen new paradigms;</li> <li>the challenges in computation and algorithm architecture caused by evermore realistic and refined systems of interest.</li> </ul> <h2 id="references--remarks">References &amp; Remarks</h2> <p><a href="https://arxiv.org/abs/2111.01008">[1]</a> A great introduction to HyperPINNs!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Fourth and last blogpost of the Heat series. It showcases how to use Physics-Informed Neural Networks (PINNs) to solve partial differential equations (in particular, the Heat equation) for arbitrary initial/boundary conditions and domain geometries.]]></summary></entry><entry><title type="html">Heat #3 - The Heat Equation (and a Physics-Informed Neural Network)</title><link href="https://manuelmlmadeira.github.io/blog/2022/heat-3-PINN/" rel="alternate" type="text/html" title="Heat #3 - The Heat Equation (and a Physics-Informed Neural Network)"/><published>2022-03-22T18:39:00+01:00</published><updated>2022-03-22T18:39:00+01:00</updated><id>https://manuelmlmadeira.github.io/blog/2022/heat-3-PINN</id><content type="html" xml:base="https://manuelmlmadeira.github.io/blog/2022/heat-3-PINN/"><![CDATA[ <p><strong>Authors</strong>: Manuel Madeira, David Carvalho</p> <p><strong>Reviewers</strong>: F√°bio Cruz</p> <p><em>This blog post was developed while working at <a href="https://inductiva.ai/">Inductiva Research Labs</a>.</em></p> <hr/> <p>As our scientific and technological needs evolve, it is not unrealistic to expect problems depending on solving <em>billions</em> of partial differential equations (PDEs) with hundreds of variables and defined on high-dimensional spaces with hard boundaries constraints.</p> <p>Can Deep Learning lend us a hand in this regard? Let‚Äôs find out!</p> <p>Employing a disruptive ethos, in this post we will <em>hopefully</em> convince you that there is a whole lot of potential stemming from <strong>Deep Learning</strong> (DL) methods in tackling this issue.</p> <p>In recent years, huge developments in Machine Learning have been triggered by ever-enhancing computational infrastructure and modeling sophistication.</p> <p>In fields such as Computer Vision or Natural Language Processing, Deep Learning (the sub-field of Machine Learning that handles Neural Networks (NNs)) has evolved as to allow for huge advances in what can be done.</p> <p>In fact, three extremely powerful points assure any researcher that NNs can be leveraged in their field:</p> <ul> <li>the capability NNs have to succeed handling extremely high-dimensional spaces (for which we have motivated bottleneck-level issues due to the <em>curse of dimensionality</em> will almost invariably arise);</li> <li>theoretical guarantees of success, namely due to the <em>Universal Approximation Theorem</em> [1];</li> <li>better still, all the DL software artillery available has dramatically lowered the entry barrier for deploying NNs frameworks. It has never been this easy to set up sophisticated models with a handful of lines of code!</li> </ul> <p>It is no surprise that NNs started invading other territories and <strong>Fundamental Sciences are no exception.</strong></p> <h2 id="deep-learning-for-partial-differential-equations">Deep Learning for‚Ä¶ Partial Differential Equations?</h2> <p>The underlying laws behind most natural sciences rely on equations dictating how certain quantities vary in time and space. Naturally, we must wonder:</p> <blockquote> <p>Can we possibly solve such an abstract problem like a Partial Differential Equation (PDE) with the aid of NNs?</p> </blockquote> <p>To answer this question, let‚Äôs first formalize a bit. In PDEs, the unknowns are functions which depend on various variables and are defined over some domain. <em>Solving the PDE (or systems thereof)</em> entails finding such functions.</p> <p>For a single real-valued PDE with $n$ spatial variables $r_i$ and a single temporal variable $t$, we are after a function $u: \Omega \in \mathbb{R}^{n+1} \rightarrow \mathbb{R}^d$ which satisfies:</p> \[\mathcal{F} \left[u, \frac{\partial u}{\partial r_i}, \frac{\partial u}{\partial t}, \ldots \right] = 0,\] <p>where $d$ sets thedimensionality of the output. <br/> $\mathcal{F}$ is a <em>differential operator</em> ‚Äî an operator which may depend on those variables but also on the trial solution itself and, most importantly, on its <em>derivatives</em>. In general, these derivative can involve any order term.</p> <p>Our hope is thus to find a NN whose output trial solution can mimic the actual PDE solution.</p> <h2 id="deep-advantages">Deep Advantages</h2> <p><em>Guess what</em>? As parametric function approximators, NNs are particularly well tailored for this problem.</p> <p>If you recall what we discussed, many limiting issues are found when numerically solving a PDE via a classical method by <strong>discretizing</strong> the domain ‚Äî usually performed with Finite Difference or Finite Element Methods. Ideally, we would like to circumvent these problems.</p> <p>As we will soon see, PINNs (Physics-Informed Neural Networks) address most of the previously mentioned limitations of the classical methods:</p> <ul> <li>they are <strong>mesh-free</strong>, making the interpolation for non-mesh points no longer a concern. Once trained, PINNs can be evaluated at any point of the domain of interest. The way the training is performed can also be adjusted so that the function-valued solution we seek can be found in very efficient ways;</li> <li>there is no such thing as <strong>error buildup</strong> or <strong>instability</strong> coming from the iterative nature and discretization imposed by the classical methods.</li> </ul> <p>For now, it‚Äôs clear ‚Äî we should express a solution of the PDE as a NN.</p> <p>Buckle yourself up and let the fun begin!</p> <h2 id="physics-informed-neural-networks-pinns">Physics-Informed Neural Networks (PINNs)</h2> <p>We will showcase you one of the hottest approaches to tackle PDEs from a DL perspective ‚Äî Physics-Informed Neural Networks (PINNs) [2,3].</p> <p>In what way does this architecture differ from more conventional NN models? Well, firstly we:</p> <ul> <li>try to approximate the function solution to the PDE through a NN that fits some data points that are provided. Typically, these are points set by boundary and initial conditions but they can also be in the interior of the domain;</li> <li>constrain the space of functions from which the NN can learn by penalizing solutions whose partial derivatives do not satisfy the desired PDE.</li> </ul> <p>The first point is commonplace in DL approaches. This simply entails a ‚Äòsupervised‚Äô minimization procedure between the output and the <em>ground truth</em> set by the conditions. This <em>flavor</em> of architecture is normally termed <strong>vanilla</strong>.</p> <p>The whole novelty and potential of PINNs stem from the second term, which is the one that fundamentally deviates from the vanilla DL by regularizing NN guesses.</p> <p>It‚Äôs this aspect that coins these NNs as <em>Physics-informed</em>: from all the NNs that we could learn, we are imposing an inductive bias towards those that verify the <em>Physics</em> of the system at hand (<em>i.e.</em> what the conditions expressed in the PDE enforce).</p> <p>The idea of <em>Physics</em> here is a bit misleading. PINNs do <strong>not</strong> have to pertain to physical systems. By Physics, it is assumed that some strong principle or law must be held across the system. Curiously enough, these tend to be expressed precisely with PDEs!</p> <h2 id="training-the-pinn">Training the PINN</h2> <p>But how can we obtain a NN that we are confident has mimicked the <em>actual</em> solution of the PDE taking into consideration the two aforementioned notions?</p> <p>Needless to say, there are <em>many</em> answers, depending on the problem at hand. For instance, we could</p> <ul> <li>try to construct (activation) functions that necessarily respect the differential equation;</li> <li>if, say, the PDE were linear, use linear combinations of solutions so to respect the PDE;</li> <li>work in the spectral (frequency) domain ‚Äî where the derivatives could be tight by some relation ‚Äî and consequently an inverse fast Fourier transform could be used to map the solution to the direct domain.</li> </ul> <h3 id="a-clever-loss-function">A Clever Loss Function</h3> <p>The answer that requires minimal change though is to tweak the way the loss function is computed. This <em>objective</em> function $\mathcal{L}$ is parametrized by tunable parameters $\mathbf{\theta} = (\theta_1, \theta_2, ‚Ä¶)$ and dictated by the model. In order to <strong>learn</strong> the solution, this loss must be <strong>minimized</strong>.</p> <p>We want to make the loss as small as possible. Once we‚Äôre in that regime, we can be fairly sure our trial output is <em>somehow</em> close to the <em>true</em> solution and we can say our model has <strong>learnt</strong> how to solve the PDE.</p> <p>Tracking the behavior of $\mathcal{L}$ allows the NN to adjust its internal parameters towards the values that lead to a smaller loss and thus to the best approximation of the PDE output by the NN.</p> <p>Considering this motivation ‚Äî admittedly more mathematical ‚Äî we can now formulate the impact of each term on the overall loss through an individual term:</p> <ul> <li> <p><strong>Output values loss</strong> $\mathcal{L}<em>\mathrm{data}$ ‚Äî any loss that penalizes a NN that does not verify the value function at the several prescribed points. For instance, we can consider a Mean Squared Error function between the values output by the NN and the target ones (which we know _a priori</em>):</p> \[\mathcal{L}_\mathrm{data} = \frac{1}{N} \sum_{i=1}^N \|\hat{u}(t^i, r^i_1, \ldots, r^i_n) - u^i\|^2,\] <p>where $\hat{u}(\ldots)$ is the NN estimate and $u^i$ is the <em>ground truth</em> value of $u$ at the point $(t^i, r^i_1, \ldots, r^i_n)$.</p> <p>Typically, the $N$ points used to compute $\mathcal{L}_\mathrm{data}$ are drawn from the initial and the boundary conditions, but this is <em>not</em> strict ‚Äî those could be any points!</p> </li> <li> <p><strong>PDE ‚ÄúRegularization‚Äù</strong> $\mathcal{L}_\mathrm{PDE}$ ‚Äî a term which penalizes models that do not satisfy the PDE. To achieve this, we can use a little trick. Since we wrote our PDE in the form $\mathcal{F}[u, \ldots]=0$, we want the left-hand side to be as close to $0$ as possible. We can then simply make sure our mimicking function $\hat{u}$ yields the lowest loss:</p> \[\mathcal{L}_\mathrm{PDE} = \frac{1}{N} \sum_{i=1}^N \|\mathcal{F}[\hat{u}(t^i, \mathbf{r}^i)]\|^2.\] <p>Easier said than done. This is a colossal task for each parameter instantiation of our NN (hundreds of billions of parameters may have to be recalibrated!)</p> <p>However, we can benefit from the auto-differentiation artillery that has been developed for NNs: the available packages allow for a straightforward and efficient way of obtaining the partial derivatives with respect to the many model inputs.</p> <p>In contrast to the first point, this loss term can be applied to <em>any</em> point, given that we do not require prescribed target values.</p> </li> </ul> <p>We are now in conditions to obtain a total loss term by summing both loss terms:</p> \[\mathcal{L}_\mathrm{PINN} = \mathcal{L}_\mathrm{data} + \mathcal{L}_\mathrm{PDE}.\] <p>Even though both terms were weighted evenly, this is not a requirement. Actually, it is quite common to find this loss defined as a convex combination of those two terms, where the weighting coefficient is an extra hyperparameter requiring appropriate fine-tuning.</p> <p>But putting these and other technicalities aside, this is pretty much what you need to know about PINNs to follow the rest of this post!</p> <div style="text-align: center;"> <img style="width:60%;" src="../../../assets/img/blogposts/heat3/PINN_sketch.png"/> </div> <p><strong>Fig. 1</strong>: The PINN we used to solve the 2D Heat Equation consists of two parts: firstly, by updating the NN‚Äôs weights $\mathbf{W}$ and biases $\mathbf{b}$ we can minimize the residual difference between the model and the prescribed values at some selected points; then the NN output is also optimized so that $|\mathcal{F}[u, \ldots] |$ gets as close to 0 as possible. Credits: David Carvalho / Inductiva</p> <h3 id="idrlnet-a-pinns-library">IDRLnet: a PINNs library</h3> <p>To set a DL algorithm, we use <a href="https://idrlnet.readthedocs.io/en/latest/index.html">IDRLnet</a>, a PINNs library built on Pytorch [4]. To the best of our knowledge, it is one of the most versatile open-source PINNs library.</p> <p>The IDRLnet library has an architecture based on three different types of nodes:</p> <ol> <li><strong><code class="language-plaintext highlighter-rouge">DataNodes</code></strong> - these are essentially domains that are obtained from Geometric Objects (also provided by the library), from where it is possible to sample the points used to train the PINN;</li> <li><strong><code class="language-plaintext highlighter-rouge">NetNodes</code></strong> - these are abstracted from neural networks, <em>i.e.</em> the architecture, training hyperparameters, and everything else related to the NN used are defined in this node;</li> <li><strong><code class="language-plaintext highlighter-rouge">PDENodes</code></strong> - these contain all the information related to the PDE we wish to solve. Contrarily to NetNodes, PDENodes do not have trainable parameters.</li> </ol> <p>Fine ‚Äî we are ready to try it out on our new favorite PDE: the 2D Heat Equation!</p> <h2 id="hot-nn-coming-through">Hot NN Coming Through!</h2> <p>To keep things simple, let us start with the exact same situation as the one presented for FDM:</p> <div style="text-align: center;"> <img style="width:40%;" src="../../../assets/img/blogposts/heat3/BIC.png"/> </div> <p><strong>Fig. 2</strong>: The boundary and initial conditions used throughout the Heat series. Energy is pumped from the top edge onto an initially completely cold 2D plate. Credits: David Carvalho / Inductiva.</p> <p>A very simple domain was chosen ‚Äî a regular 2D square plate. We must then consider points of the form $(t, \mathbf{r}) = (t, x, y)$. The temperature $u(t, \mathbf{r})$ must satisfy <em>the 2D Heat Equation</em>:</p> \[\left[ \frac{\partial}{\partial t} - D \left( \frac{\partial^2}{\partial x ^2} + \frac{\partial^2}{\partial y ^2} \right) \right]u(t,x,y)= 0\] <p>Recall that the hot edge was kept at the <em>maximal</em> temperature ($1\;^\mathrm{o}C$), while the remaining boundaries at the <em>minimal</em> temperature ($-1\;^\mathrm{o}C$). Initially, at $t=0$, all points were kept at the minimal temperature too.</p> <p>This prescription allows us to compute the loss term $\mathcal{L}<em>{\mathrm{data}}$. As for the regularization, the loss term $\mathcal{L}</em>{\mathrm{PDE}}$ is:</p> \[\mathcal{L}_\mathrm{PDE} = \frac{1}{N} \sum_{i=1}^N \left\|\left[ \frac{\partial}{\partial t} - D \left( \frac{\partial^2}{\partial x ^2} + \frac{\partial^2}{\partial y ^2} \right) \right]\hat{u}(t^i,x^i,y^i) \right\|^2= 0.\] <h2 id="time-to-heat-start">Time to Heat [Start]</h2> <p>You can find our code <a href="https://github.com/inductiva/blog_code_snippets">here</a> and run your very first PINN! Run this experiment through the following instruction in the command line:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python heat_idrlnet.py --max_iter=10000 --output_num_x=500 --output_num_y=500 --colorbar_limits=-1.5,1.5
</code></pre></div></div> <p>The flags used trigger the following instructions:</p> <ul> <li><code class="language-plaintext highlighter-rouge">max_iter</code> defines the total number of training epochs;</li> <li><code class="language-plaintext highlighter-rouge">output_num_x</code> and <code class="language-plaintext highlighter-rouge">output_num_y</code> define the discretization along the x-axis and y-axis, respectively, of the grid in which we infer results;</li> <li><code class="language-plaintext highlighter-rouge">colorbar_limits</code> defines the range of the colorbar used.</li> </ul> <p>For illustrative purposes, we set the diffusivity constant to $D=0.1$ throughout the entire post.</p> <p><strong>Disclaimer:</strong> our code is able to accommodate some extra complexity that is not needed in this post. For now though, we will not dive in detail but let the magic happen in the next section üòé.</p> <h2 id="classical-vs-nn--the-fight-begins">Classical vs NN ‚Äî the fight begins</h2> <p>To see how well our NN-based framework handles the task, we can compare the NN output to the one generated from the classical Finite-Differences algorithm:</p> <p>Let‚Äôs plot the output obtained with the FDM (a classical algorithm) [top] and a PINN we trained [middle], as well as the error $\text{Error} = |u_{\rm FDM} - u_{\rm PINN}|$ [bottom]. This error plot can be easily computed by running the provided <code class="language-plaintext highlighter-rouge">heat_error.py</code> python script.</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat3/fdm.mp4" type="video/mp4"/> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat3/idrlnet_10000epochs.mp4" type="video/mp4"/> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat3/error_10000epochs.mp4" type="video/mp4"/> </video> </div> <p><strong>Fig. 3</strong>: Comparison of the results obtained via a classical algorithm (a FDM)[top], a DL algorithm (the PINN computed with IDRLnet)[middle] and the absolute value of their difference [bottom]. They seem very similar! Indeed, an inspection of the error shows global convergence. Credits: Manuel Madeira / Inductiva</p> <p>Wow ‚Äî this looks rather good!</p> <p>The NN output approximates quite closely the results obtained with FDM. There are some deviations mainly in the initial instants and then in the upper corners. Actually, these are the regions where sharper transitions of the solution function are found, and it is thus natural that our PINN has more difficulty to fit correctly there.</p> <p>This certainly seems hopeful ‚Äî but an inquisitive mind like yours must be wondering about <em>why</em> this PINN worked.</p> <h2 id="how-long-should-we-train">How long should we train?</h2> <p>In order to train the PINN, a rather large number of epochs ($N_{\rm epochs}=10000$) was used for training.</p> <p>This can make us think: just like the classical algorithm had to be tuned so no nonsensical estimates were output, there must be some suitable tuning to assure us the algorithm can indeed approximate the solution.</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat3/error_100epochs.mp4" type="video/mp4"/> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat3/error_1000epochs.mp4" type="video/mp4"/> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat3/error_10000epochs.mp4" type="video/mp4"/> </video> </div> <p><strong>Fig. 4</strong>: Difference between the PINN output after training with different number of epochs and its respective classical output (serving as a benchmark). We can see a substantial error for few epochs ($N_{\rm epochs} = 100$) [top]. If we ramp up 10-fold ($N_{\rm epochs} = 1000$), the error is essentially gone [middle] and even further suppressed for $N_{\rm epochs} = 10000$ [bottom]. For this regime, both the classical and DL algorithms provide essentially the same estimate. Credits: Manuel Madeira / Inductiva.</p> <p>The results are just as we expected: the higher $N_{\rm epochs}$, the better the NN learns. In this particular case, it seems that by around $N_{\rm epochs} \approx 1000$ it already leads to appropriate learning. In general, tuning this parameter can be costly (moreso if performed in a brute force fashion).</p> <p>Specific knowledge of the PDE or domain in question may make our lives easier!‚Ä¶</p> <h2 id="customized-training">Customized training</h2> <p>The versatility of the PINNs in focusing on different regions differently begs a question of the utmost relevance:</p> <blockquote> <p>Are we favoring or penalizing regions that require a different level of refinement?</p> </blockquote> <p>Fortunately, via <em>TensorBoard</em>, it is possible to track various loss terms throughout the training procedure by using the command</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensorboard --logdir [insert path to "network_dir"]
</code></pre></div></div> <p>With it, we can then see the effect of extending the training procedure. For this, we set different <strong>numbers of epochs</strong> and see how the output is impacted by this choice.</p> <div style="text-align: center;"> <img style="width:95%;" src="../../../assets/img/blogposts/heat3/learning_across_epochs.png"/> </div> <p><strong>Fig. 5</strong>: Learning curves (in logarithmic scale) for different sub-domains. We can see that, even though all terms have different behavior, they eventually converge to exceedingly small values. Credits: Manuel Madeira / Inductiva.</p> <p>You can see how the loss for various subdomains (the captions should be clear to follow) changes as training takes place.</p> <p>The training of PINNs (or generally with NNs), is <strong>not</strong> a completely trivial task. In fact, from the learning curves above, it is clear that if we stopped our training too early on, our results would be necessarily <strong>worse</strong>.</p> <h2 id="the-ghost-of-overfitting">The ghost of <em>overfitting</em></h2> <p>We typically find overfitting whenever we train our model excessively with a training dataset ‚Äî ultimately providing a strict fitting across the given data points, or when training using a closed dataset (by <em>not</em> using randomly-generated data).</p> <p>This leads to a huge drawback: <strong>the model loses its generalization capability</strong>.</p> <p>In other words, when we present the model to new <strong>unseen</strong> data still from the same distribution that generated the training dataset, the model will have <em>weak</em> predicting power for those points and thus a <strong>much higher</strong> loss rate.</p> <p>A validation set of data points is typically kept in parallel to the NN training procedure to ensure, through periodic checks, that we do not enter such an overfitting regime.</p> <p>In this case, we do not have to worry about the possibility of overfitting for two main reasons:</p> <ul> <li>firstly, we are sampling a different set of data points in each epoch, <em>i.e.</em>, our training set is changing from epoch to epoch</li> <li>secondly, we are considering a regularization term in our loss (recall $\mathcal{L}_\mathrm{PDE}$).</li> </ul> <p>Furthermore, we see that points coming from the boundary conditions are the most troublesome ones ‚Äî due to their higher loss. To contrast this, FDM implementations have those points directly <em>hard-coded</em> onto the final solution.</p> <h2 id="learning-rate">Learning Rate</h2> <p>Another typical issue impacting the performance of PINNs (and NNs in general) is the choice of the learning rate $\alpha$.</p> <p>This is a <em>hyperparameter</em> ‚Äî a variable which pertains to the structure of the model itself. The update of the NNs parameters is performed by applying optimization algorithms, thus known as <em>optimizers</em>.</p> <p>The way in which the learning rate is exploited in these optimizers varies. However, we can intuitively think about it as the magnitude of the step given in the parameters space in each iteration.</p> <p>Striking a balance between <em>speed</em> and <em>accuracy</em> is not only a vital part of setting the model but also a delicate task on its own.</p> <p>A <strong>larger</strong> learning rate allows us to have larger updates in the parameters and thus <strong>faster</strong> progress but at the same time it may lead to <strong>large instabilities</strong> in the convergence process. In extreme instances, we may never be in conditions to access the optimal regions.</p> <p>To see this, let us see the effect of using larger and smaller learning rates $\alpha$ than the one chosen in the previous experiment.</p> <div style="text-align: center;"> <img style="width:95%;" src="../../../assets/img/blogposts/heat3/big_lr.png"/> <img style="width:95%;" src="../../../assets/img/blogposts/heat3/small_lr.png"/> </div> <p><strong>Fig. 6</strong>: Learning curves (in logarithmic scale) for two different learning rates. Note that a very large rate $\alpha = 0.1$ leads to curves that never get smaller and so the NN is bound to fail [top]. Using a smaller rate ($\alpha=0.0001$) [bottom], although takes more time to run, indeed leads to increasingly smaller losses. Credits: Manuel Madeira / Inductiva.</p> <p>On the one hand, for $\alpha = 0.1$, the model runs rather quickly but the learning curve stagnates quickly since further progress will not get us closer to the optimal minima of the loss function ‚Äì we simply keep meandering in parameter space!</p> <p>Contrary to this regime, a small rate $\alpha=0.0001$ may take more time but allows the model to eventually have ever-smaller losses.</p> <p>Balancing performance, accuracy and computation resources surely is a sophisticated feat!</p> <h2 id="there-is-art-in-dl">There is art in DL</h2> <p>The lesson we learnt here is that indeed NNs can be adequately trained to yield estimates of PDE solutions <strong>without explicit computation of the PDE</strong>.</p> <p>For that though, we must be crafty ‚Äî choices like the learning rate and the number of epochs <strong>directly impact</strong> the performance of the PINN.</p> <p>The fine-tuning of hyperparameters is not something set on stone and requires subtlety and exploration to be successful. The technical aspects and their formulation are still open topics and intensively studied in the DL+ML communities.</p> <p>Just as with FDMs we had to fine-tune the discretization parameters in order to ensure the stability of the method, the same must be considered for PINNs ‚Äî they are not a panacea! Even though it offers an edge in streamlining the computation of the estimate, extra care is still needed!</p> <p>NN architectures come in many flavors and recipes and may be improved in many intuitive ways. As a general question, we can wonder:</p> <blockquote> <p>Should we take some inspiration from the classical methods and explore a direction where the boundary (and initial) conditions are hard-constrained in the learning problem (<em>e.g.</em> verified by construction)?</p> </blockquote> <p>This is one of the questions that we are tackling at Inductiva ‚Äî and we will show you more as progress is made üòõ!</p> <h2 id="next-episode">Next episode</h2> <p>In the next (and final!) section of our tutorial, we will precisely unveil some of the aspects that can make our problem harder to solve but also <strong>more realistic</strong>.</p> <p>In particular, we will show you how to extend this vanilla version of PINNs to be able to:</p> <ul> <li>deal with complex geometries (rather than a simple square plate);</li> <li>solve a PDE to more than one instance of the boundary conditions without having to retrain the whole model again.</li> </ul> <p>Neural Networks sure sound exciting, right? Stay tuned üå∂Ô∏è!</p> <h2 id="references">References</h2> <p><a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">[1]</a> A bit more about the mathematical foundation for the suitabillity of NNs to model challenging maps from input to output. <br/> <a href="https://arxiv.org/abs/1711.10561">[2]</a> Defining papers outlining the architecture and ideas of PINNs in the Literature. <br/> <a href="https://www.sciencedirect.com/science/article/pii/S0021999118307125">[3]</a> Yet another influential paper on PINNs. <br/> <a href="https://idrlnet.readthedocs.io/en/latest/">[4]</a> Check out the documentation of the IDRLnet library here.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Third blogpost of the Heat series. It shows how to solve the Heat equation using a Physics-Informed Neural Network (PINN).]]></summary></entry><entry><title type="html">Heat #2 - Solving the Heat Equation with Finite-Differences</title><link href="https://manuelmlmadeira.github.io/blog/2022/heat-2-finite-differences/" rel="alternate" type="text/html" title="Heat #2 - Solving the Heat Equation with Finite-Differences"/><published>2022-03-03T01:00:00+01:00</published><updated>2022-03-03T01:00:00+01:00</updated><id>https://manuelmlmadeira.github.io/blog/2022/heat-2-finite-differences</id><content type="html" xml:base="https://manuelmlmadeira.github.io/blog/2022/heat-2-finite-differences/"><![CDATA[ <p><strong>Authors</strong>: Manuel Madeira, David Carvalho</p> <p><strong>Reviewers</strong>: F√°bio Cruz, Augusto Peres</p> <p><em>This blog post was developed while working at <a href="https://inductiva.ai/">Inductiva Research Labs</a>.</em></p> <hr/> <h2 id="discretizing-our-domain">Discretizing our domain</h2> <p>Deciding how to discretize the domain is also <em>far</em> from being set on stone: for a given instantiation of a PDE problem, it is typically one of the most complex steps in its resolution. Squares are cool and all but what if we want to simulate the Heat Equation in a mug or a pan?</p> <p>We‚Äôll make our lives easier (for now!) by using a <em>regular grid</em> to represent the domain. With the aid of the cube:</p> \[(t,x,y) \in [t_\mathrm{min}, t_\mathrm{max}] \times [x_\mathrm{min}, x_\mathrm{max}] \times [y_\mathrm{min}, y_\mathrm{max}]\] <p>in a regular grid with $N_t$, $N_x$ and $N_y$ points along the $t$, $x$ and $y$-axis, respectively, we can set their step intervals, defined by their regular spacing along their respective dimension:</p> \[(\Delta t, \Delta x, \Delta y) = \left( \frac{t_\mathrm{max} - t_\mathrm{min}}{N_t-1}, \frac{x_\mathrm{max} - x_\mathrm{min}}{N_x-1}, \frac{y_\mathrm{max} - y_\mathrm{min}}{N_y-1} \right)\] <p>Consequently, input points $(t,x,y)$ become discretized as $(t_k, x_i, y_j)$ and associated with a node $[k,i,j]$. Here,</p> \[(t_k, x_i, y_j) = \left\{ \begin{matrix} t_k = t_0 + k \Delta t \ \ \ \ \ \ \ \ \ \ \ \ \text{for} \ 0 \leq k \leq N_t-1 \\ x_i = x_0 + i \Delta x \ \ \ \ \ \ \ \ \ \ \ \text{for} \ \ 0 \leq i \leq N_x-1 \\ y_j = y_0 + j \Delta y \ \ \ \ \ \ \ \ \ \ \ \text{for} \ 0 \leq j \leq N_y-1 \end{matrix} \right.\] <p>It is in this <em>pixelated</em> world we will express how heat will diffuse away‚Ä¶</p> <h2 id="from-continuous-to-discrete-derivatives">From continuous to discrete derivatives</h2> <p>So, we now need to express a differential operator in a finite, discretized domain. How exactly do we <em>discretize</em> such abstract objects, like the (univariate) derivative $f_x(x)$:</p> \[f_x(x) = \underset{\Delta x \to 0}{\mathrm{lim}} \; \frac{f(x+ \Delta x) - f(x)}{\Delta x}.\] <p>or a partial derivative with respect to, say, a coordinate $x_j$:</p> \[f_{x_j}(x_1, \dots, x_j,\ldots,x_N)= \underset{\Delta x_j \to 0}{\mathrm{lim}} \; \frac{f(x_1, \dots, x_j + \Delta x_j, \dots ,x_N) - f(x_1, \ldots,x_j, \dots, x_N)}{\Delta x_j}\] <p>Being on a grid means that we require information on a <em>finite</em> number of points in the domain. The FDM philosophy is to express (infinitesimal) differentials by <strong>(finite) differences</strong> between any nodes.</p> <p>The <em>continuous</em> thus becomes <em>discrete</em> ‚Äî the differential operator $\mathcal{L}(t,x,y)$ becomes a function-valued discrete <em>grid</em> $\equiv \mathcal{L}[k,i,j] = \mathcal{L}(t_k,x_i,y_j) $. Resulting from this, so will our solution be output as the grid $u[k,i,j]$.</p> <p>In essence, we must find an algorithm which can propagate the constrains set by the PDE from the initial conditions as faithfully as possible along the grid. In a more specific sense, you might wonder:</p> <blockquote> <p>How to approximate $\mathcal{L}$ <em>reasonably</em> to obtain $u[k,i,j]$ for all nodes $[k,i,j]$?</p> </blockquote> <p>This is exactly what the FDM solver will accomplish. For that, it has to approximate the differential operators ‚Äî $u_{t}[k,i,j]$, $u_{xx}[k,i,j]$ and $u_{yy}[k,i,j]$ ‚Äî <strong>at all nodes</strong> $[k,i,j]$.</p> <p><em>Goodbye operators, hello grids!</em></p> <h2 id="finite-difference-method">Finite Difference Method</h2> <p>Approximating differentials on a discrete set is also not a recipe set on stone. Let us look at two Finite Difference approximations to a function of a single variable $x$:</p> <div style="text-align: center;"> <img style="width:40%;" src="../../../assets/img/blogposts/heat2/derivatives.png"/> </div> <p><strong>Fig. 1</strong>: Approximation schemes of the derivative of $f(x)$ at a grid point $x_i$ using - (left) <em>forward</em> and (right) <em>centered</em> schemes. Note that either method differs from the <em>actual</em> value $f_x(x_i)$, given by the slope of the solid straight line. Credits: David Carvalho / Inductiva.</p> <p>These will estimate the true instantaneous rate via neighboring differences. Expectedly, an error will be incurred in the process.</p> <p>The price to pay in ‚Äúdropping‚Äù the limit results in a 1st-order approximation, meaning this estimate scales <em>linearly</em> with the spacing $\Delta x$. For our approximation to have a chance to succeed, we better sample the $x$ axis with a high $N_x$!</p> <h2 id="choosing-how-to-spread-heat">Choosing how to spread heat</h2> <p>Given a particular PDE, different FDMs would iterate differently over the grid <strong>node by node</strong> to obtain estimates of the differential operators. <br/> So, fixing <em>a scheme</em> is (again) delicate task.</p> <p>You guessed it right - for this problem, we will use the 2 approximation schemes we showed before to evaluate the differential operators. They are combined in the <em>FTCS scheme</em>:</p> <ul> <li> <p><strong>Forward in Time</strong> - the time partial derivative $u_t$ uses the <em>forward</em> 1st order differences</p> \[u^{k, i, j}_t = \frac{u^{k+1, i, j} - u^{k, i, j}}{\Delta t}\] </li> <li> <p><strong>Centered in Space</strong> - the spatial partial derivatives $u_{xx}$ and $u_{yy}$ are are computed through a 2nd-order <em>central difference</em> by applying the centered difference <em>twice</em>: - We first approximate the (second-order) derivative (say, $u_{xx}$) via the centered difference:</p> \[u^{k, i, j}_{xx} = \frac{u^{k, i+\frac{1}{2}, j}_x - u^{k, i-\frac{1}{2}, j}_x}{\Delta x}\] <ul> <li>And then express each first-order term $u_x$ in the same fashion e.g.:</li> </ul> \[u^{k, i, j}_{x} = \frac{u^{k, i+\frac{1}{2}, j} - u^{k, i-\frac{1}{2}, j}}{\Delta x}\] <ul> <li>With a bit of rearranging, both spatial derivatives become:</li> </ul> \[\begin{split} u_{xx}^{k,i,j} = \frac{u^{k, i+1, j} - 2 u^{k, i, j} + u^{k, i-1, j}}{(\Delta x)^2} &amp;&amp; \ \ \ \ \ \ u_{yy}^{k,i,j}= \frac{u^{k,i,j+1} - 2 u^{k,i,j} + u^{k,i,j-1}}{(\Delta y)^2} \end{split}\] </li> </ul> <p>Look ‚Äî some terms are evaluated <em>outside</em> the original grid. However, you will notice that these fractional indexes only appear for the computation of intermediary constructions/variables [4].</p> <h2 id="heat-diffusion-on-a-grid">Heat diffusion on a grid</h2> <p><em>Phew</em>! That was intense but we now have all the tools to start simulating heat flow! For that, we must:</p> <ol> <li><em>discretize the domain</em> by sampling each dimension individually and considering all possible combinations of the coordinates, creating points $(t_k,x_i,y_j)$, indexed by nodes $[k,i,j]$.</li> <li><em>discretize</em> the differential operators using a FTCS scheme.</li> <li>solve for the solution grid $u[k,i,j]$ by using the <em>iteration rules</em> that propagate the solution across all nodes $[k,i,j]$.</li> </ol> <p>In this fashion, we convert the 2D Heat Equation to this <strong>difference equation</strong>:</p> \[u^{k+1, i, j} = \alpha (u^{k, i+1, j} + u^{k, i-1, j}) + \beta (u^{k, i, j+1} + u^{k, i, j-1}) + (1 - 2\alpha - 2\beta) u^{k, i, j}\] <p>where we can lump all parameters in the ratios:</p> \[\begin{split} \alpha \equiv D\frac{\Delta t}{(\Delta x)^2} \ \ \ \ \ \ &amp;&amp; \ \ \ \ \ \ \ \beta \equiv D \frac{\Delta t}{(\Delta y)^2}. \end{split}\] <p>We can understand how knowledge about the function <strong>propagates</strong> along the grid with a <em>stencil</em> by depicting which input grid points are needed to iterate the algorithm so a solution estimate may be computed at all grid points.</p> <div style="text-align: center;"> <video class="mb-0" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/fdm_animation.mp4" type="video/mp4"/> </video> </div> <p><strong>Fig. 2</strong>: A stencil used by the FTCS scheme. At a given node $[k,i,j]$, the solution is propagated forward in time as $u[k+1,i,j]$ by considering the 2-point centered derivatives in $x$ (the neighboring nodes $[k,i+1,j]$ and $[k,i-1,j]$) and in $y$ (the nodes $[k,i,j+1]$ and $[k,i,j-1]$). Credits: Augusto Peres, Inductiva.</p> <h2 id="run-our-code">Run our code</h2> <p>It‚Äôs time to play with all these concepts and see how our FDM approximation fares.</p> <p><strong>All our code can be accessed on our <a href="https://github.com/inductiva/blog_code_snippets">code snippets GitHub repository</a></strong>. There, you will find the <code class="language-plaintext highlighter-rouge">heat_fdm.py</code> file for simulations and plotting utilities in <code class="language-plaintext highlighter-rouge">/utils</code>. <br/> Have a go yourself!</p> <h2 id="the-role-of-the-d-ffusivity">The role of the $D$-ffusivity</h2> <p>We will pick a spatial discretization of, say, $500 \times 500$ points. We can now run for different thermal diffusivities and see their effect. Below you can see the temperature profiles as we increase $D$ ‚Äî first with $D=0.01$, then $D=0.1$ and finally $D=1$.</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/ftcs_d001.mp4" type="video/mp4"/> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/ftcs_d01.mp4" type="video/mp4"/> </video> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/ftcs_d1.mp4" type="video/mp4"/> </video> </div> <p><strong>Fig. 3</strong>: Role of various $D$ in the diffusion. Credits: Manuel Madeira / Inductiva.</p> <h2 id="heat-is-on-the-plate">Heat is on the plate!</h2> <p>We can reason with these results.</p> <ul> <li> <p>At the beginning (for $t=0$), heat starts developing from the only points that are not at the common temperature ($-1 \; ^\mathrm{o}C$), where temperature gradients exist. These are points nearby the hot (top) edge.</p> </li> <li> <p>From that point on, we can observe the progressive heat diffusion from the hot edge towards lower regions, until we reach a stationary state, where no further changes occur (and the system is in <strong>thermal equilibrium</strong>).</p> </li> <li> <p>Heat does not spread evenly across the plate ‚Äî its spread is faster in regions that are further from the cold regions. Given this symmetric setup, points along the central line between the edges allow for faster diffusion than regions nearby the edges. This will progressively wind down until we reach the cold edges and a paraboloid-like pattern is observed.</p> </li> <li> <p>But more importantly ‚Äî the higher the $D$, the faster this spread occurs. This <strong>directly</strong> impacts the choice of the time sampling.</p> </li> </ul> <p>So ‚Äî it seems that each <strong>internal</strong> parameter $D$ requires its own discretization setup somehow. But in what way?</p> <h2 id="setting-stability-criteria">Setting stability criteria</h2> <p>We ran our first PDE classical solver. <em>Yay!</em> However, why are we happy with the results? Clearly, we can think of potential issues, such as:</p> <ul> <li>for excessively sparse discretizations, our approximations of the differential operators will be miserably <em>undersampled</em> and consequently the trial solution will be <em>shockingly wrong</em>.</li> <li><strong>error buildup</strong> will take place as the iterative procedure tends to propagate and amplify errors between any iterations. In those cases, we say that the method is <strong>unstable</strong>.</li> </ul> <p>However, these are the extreme instances. Were we lucky? If we are using ‚Äúreasonable‚Äù discretizations, a question should make you scratch your head:</p> <blockquote> <p>How to <em>be sure</em> a certain discretization provides a trustful approximation to the PDE solution?</p> </blockquote> <p>Fortunately for us, you can notice that the discretization setup can be monitored <strong>exclusively</strong> through the ratios $\alpha$ and $\beta$! This raises the question:</p> <blockquote> <p>Can we somehow fine-tune them to ensure <em>stability</em> <em>i.e.</em> find admissible combinations of $(\Delta t, \Delta x, \Delta y)$ for a fixed $D$?</p> </blockquote> <p>Well, theoretical studies can be performed to find a restricted space of acceptable discretization setups. For our case [3], <em>Von-Neumann analysis</em> establishes a surprisingly simple <em>stability criterion</em>:</p> \[\alpha + \beta \leq \frac{1}{2}\] <p>These two constants provide a straightforward procedure to be sure that we i) are not <strong>oversampling</strong> or <strong>undersampling</strong> a dimension with respect to any other and ii) have spacings that can capture meaningful changes of the solution.</p> <p><strong>This is not a loose condition</strong>. <br/> Just to show you that we‚Äôre playing with fire, we cannot think of a more illustrative example than by ramping up that bound by a mere $2 \% $ above the theoretical maximum <em>i.e.</em> for $\alpha + \beta \approx 0.51$:</p> <div style="text-align: center;"> <video style="width:80%;" class="mb-0" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/ftcs_unstable.mp4" type="video/mp4"/> </video> </div> <p><strong>Fig. 4</strong>: In a discretization step for which $\alpha + \beta &gt; 0.5$, our algorithm is <em>bound to fail</em>. Credits: Manuel Madeira / Inductiva</p> <p>Yup: for this setup, an ‚Äúexplosion‚Äù caused by the FDM instability can be seen propagating along the trial solution ‚Äî quickly unfolding to some psychedelic burst!</p> <p>At the very beginning, some similarities to the downwards diffusion pattern ( that we just saw previously) quickly <strong>fade away</strong> as the <strong>error buildup spreads</strong> away and <strong>faster</strong> in regions with already large errors.</p> <p>It‚Äôs a race for disaster: the exponential buildup of the errors, even if sustained for only a handful of propagation steps, ensures that some nodes will experience <em>huge growth</em> (and $u$ becomes <em>unbounded</em>) ‚Äî $ u \mapsto 1 \mapsto 10^{15} \mapsto \dots 10^{80} \dots $, until the maximum computer precision is reached ‚Äî at which point its value becomes a <code class="language-plaintext highlighter-rouge">NaN</code> (Not a Number). You can see that eventually <strong>all</strong> points are not even represented on the scale (they‚Äôre white)!</p> <p>By the same token, let‚Äôs now see what happens if we‚Äôre <em>close to the edge</em> from within the admissible area. Dropping our bound by $2 \% $, let‚Äôs heat the start button for $\alpha + \beta \approx 0.49$.</p> <div style="text-align: center;"> <video class="mb-0" style="width:80%;" loop="" muted="" autoplay="" preload="auto"> <source src="../../../assets/img/blogposts/heat2/ftcs_almost_unstable.mp4" type="video/mp4"/> </video> </div> <p><strong>Fig. 5</strong>: With our parameters recalibrated, stability has been restored! Credits: Manuel Madeira / Inductiva</p> <p><em>Incredible!</em> Not even a minimal sign of instability coming through.</p> <p>We‚Äôre happy - we obtained seemingly satisfactory outputs in a reasonable way. The simulations take about $10s$ and CPUs can handle the computation just fine.</p> <p>But remember though: this is an extremely simple system‚Ä¶ We could be considering this problem in 3D or at way more complex domains and boundaries. In an extreme limit, we could be using these FDMs in highly-nonlinear equations with hundreds of variables.</p> <h2 id="to-vectorize-or-not-to-vectorize--that-is-not-the-question"><em>To Vectorize or Not To Vectorize</em> ‚Äì that is not the question</h2> <p>Understandingly, instructing how to compute many functions over a grid is not a trivial task ‚Äî issues like computation power and memory storage can be bottlenecks already for discretization setups <em>far from ideal</em>.</p> <p>We must understand how our implementation choices affect the algorithm performance and the output accuracy. In essence:</p> <blockquote> <p>How much faster can we solve the PDE without compromising the output?</p> </blockquote> <p>Using the <em>running time</em> as a metric, we‚Äôll compare 3 mainstream implementations:</p> <ul> <li> <p><strong>Nested <code class="language-plaintext highlighter-rouge">For</code> Cycles</strong>: we fill in the entries for the plate at the new time frame $u^{k+1, i, j}$, by nesting 2 <code class="language-plaintext highlighter-rouge">For</code> cycles that iterate through the $x$ and $y$ axes from the previous time frame in $u^{k, i, j}$.</p> </li> <li> <p><strong>Vectorized approach</strong>: we perform arithmetic operations on $u^{k+1,i,j}$ by considering it as a linear combination of slightly shifted versions $u^{k,i,j}$, multiplied by constants (known <em>a priori</em>), resulting in scalar multiplications and additions of matrices. <em>NumPy</em> implementations will be used.</p> </li> <li> <p><strong>Compiled approach</strong>: we can compile a vectorized approach, which allows sequences of operations to be optimized together and run at once. The code compilation was performed through <strong>JAX</strong>, using its just-in-time (JIT) compilation decorator.</p> </li> </ul> <p>The approach used is passed as a flag: <code class="language-plaintext highlighter-rouge">--vectorization_strategy=my_strategy</code>. <br/> The default is set to <code class="language-plaintext highlighter-rouge">numpy</code> and the vectorized approach is used. When set to <code class="language-plaintext highlighter-rouge">jax</code>, a compiled version of the vectorized approach is used and, when set to <code class="language-plaintext highlighter-rouge">serial</code>, the nested <em>For</em> cycles approach is used.</p> <p>After running each implementation for the same parameters, we obtain <em>wildly</em> different running times:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vectorization_strategy=serial - Time spent in FDM: 93.7518858909607s
vectorization_strategy=numpy - Time spent in FDM: 0.6520950794219971s
vectorization_strategy=jax - Time spent in FDM: 0.297029972076416s
</code></pre></div></div> <p><em>Wow!</em> By using the vectorized approach, we obtain more than a $100$-fold boost in running speed!</p> <p>This is not surprising. The vectorized approach takes advantage <strong>SIMD</strong> (Single Instruction, Multiple Data) computations, where one instruction carries out the same operation on a number of operands in parallel.</p> <p>This discrepancy is then easy to understand and a huge acceleration is observed, in constrast when ran with the nested <code class="language-plaintext highlighter-rouge">For</code> cycles, where each instruction is executed <strong>one at a time</strong>.</p> <p>We are not done yet. By compiling the vectorized approach, we obtain yet another $3$-fold speed-up, further improving our metric! This improvement would be even more significant if we ran the compiled code in GPUs (currently, JAX still only supports CPU in Apple Silicon machines).</p> <p>Computation resources are finite (and costly). Despite the huge boost granted by the compiled vectorized approach, this methodology improved our task by 2 orders of magnitude. Is that enough when dealing with far more complex PDEs in higher dimensions?</p> <h2 id="how-far-can-we-push-classical-methods">How far can we push classical methods?</h2> <p>Despite our pleasant experience with a low-dimensional problem on a regular geometry, we should be careful thinking ahead. Classical methods for solving PDEs have long been recognized for suffering from some significant drawbacks and we will comment on a few:</p> <h2 id="meshing">Meshing</h2> <p>Meshing becomes a daunting task in more complex geometries. Different discretization setups can always be chosen but once one is fixed, we have limited ourselves to computations <strong>on the nodes generated by that particular setup</strong>. We can no longer apply <em>arbitrary smaller</em> steps to approximate the differential operators.</p> <p>Worse still ‚Äî depending on the behavior of the differential operator, we might need to focus on certain regions more than others. It‚Äôs unclear how these choices impact the quality of the approximations.</p> <p>Discretizing the problem also creates a bias in favor of the grid points with respect to all others.</p> <blockquote> <p>But what if there the PDE behaves differently outside the mesh?</p> </blockquote> <p>The best we can do is to infer a solution through <strong>interpolation</strong> but this is admittedly <strong>unsatisfactory</strong>.</p> <h3 id="scaling">Scaling</h3> <p>Classical algorithms scale <strong>terribly</strong> with the PDE dimensions and grid refinements.</p> <p>For all the Complexity lovers out there ‚Äî you know that sweeping the regular grid $[k,i,j]$ step-by-step requires algorithms of order $\mathcal{O}(N_t N_x N_y)$. <br/> By simply refining our grid by $5$ times, we scale the number of nodes by a whopping factor $5^3 = 125$ (!!!).</p> <p>Similar bad news are expected as we consider simulations on PDEs for higher dimensions. For a problem of 3D heat diffusion, the complexity grows as $\mathcal{O} (N_t N_x N_y N_z)$ and naturally, $n$ dimensions scale as $\mathcal{O}(N^d)$ ‚Äî an <em>exponential scaling</em> of the algorithm with dimension.</p> <h2 id="feasibility">Feasibility</h2> <p>Even though we can rely on stability conditions, this does <strong>not</strong> mean we can deploy them ‚Äî it may be computationally very, very, ‚Ä¶, heavy or simply impossible.</p> <p>As we saw, simulating a medium with <em>higher</em> $D$ requires a <em>finer</em> time discretization. For some instances, satisfactory $\Delta t$ might be too small to be practical and, in the extreme case, downright untenable!</p> <p>This comes at a bitter price and is a <strong>major</strong> drawback of the FDM.</p> <h2 id="machine-learning">Machine Learning?</h2> <p>PDE simulations must be <strong>as fast and light as possible</strong>. But can classical methods, in general, get <em>fast enough</em>?</p> <p>We can look farther ahead. Consider something extreme (but likely). Imagine our task is to find parameters that lead to optimal performance for some problem.</p> <p>Let us assume we have access to the theoretically fastest PDE classical solver, foolproof to instabilities and extremely efficient in terms of memory and processing.</p> <blockquote> <p>If hundreds, millions, ‚Ä¶, billions of intermediate PDE calculations had to be performed with that algorithm, could we afford the computational budget? Would the time elapsed be reasonable?</p> </blockquote> <p><strong>Certainly not.</strong> It is not difficult to estimate runtimes <em>comparable to our lifetime</em> (run across millions of processors!) to achieve some real-life optimization tasks.</p> <p>This would <em>forbid</em> us from iterating across the parameter space fast enough ‚Äî <em>Game over</em>.</p> <p>Efficiency in classical methods <strong>won‚Äôt</strong> provide the scaling we need to overcome this barrier. <br/> <strong>A new paradigm is needed.</strong></p> <p>Fortunately, a large class of algorithms can handle this <strong>curse of dimensionality</strong> exceptionally well. <em>Can you guess what we are talking about?</em> <br/> Yup, <em>Deep Learning</em>!</p> <p>In the following posts of this series, we will address how we can leverage on those approaches to possibly overcome these issues raised by classical methods.</p> <p><em>Until then, don‚Äôt melt away!</em> √•</p> <h2 id="references--remarks">References &amp; Remarks</h2> <p><a href="https://www.machinedesign.com/3d-printing-cad/fea-and-simulation/article/21832072/whats-the-difference-between-fem-fdm-and-fvm">[1]</a> More on this FEM and FDM possible similarity here. <br/> <a href="https://levelup.gitconnected.com/solving-2d-heat-equation-numerically-using-python-3334004aa01a">[2]</a> The FDM-based approach we present is loosely following this excellent example that inspired us.<br/> <a href="https://www.uni-muenster.de/imperia/md/content/physik_tp/lectures/ws2016-2017/num_methods_i/heat.pdf">[3]</a> More details of the derivation of this expression can be found here (pages 11/12). <br/> <strong>[4]</strong> There are more complex cases where it is convenient to define such <em>staggered grids</em> with fractional offsets to the original (e.g for Maxwell‚Äôs Equations).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Second blogpost of the Heat series, on how to solve the Heat equation using a Finite-Differences method.]]></summary></entry><entry><title type="html">Heat #1 - The Heat Equation</title><link href="https://manuelmlmadeira.github.io/blog/2022/heat-1-heat-equation/" rel="alternate" type="text/html" title="Heat #1 - The Heat Equation"/><published>2022-02-14T18:39:00+01:00</published><updated>2022-02-14T18:39:00+01:00</updated><id>https://manuelmlmadeira.github.io/blog/2022/heat-1-heat-equation</id><content type="html" xml:base="https://manuelmlmadeira.github.io/blog/2022/heat-1-heat-equation/"><![CDATA[ <p><strong>Authors</strong>: Manuel Madeira, David Carvalho</p> <p><strong>Reviewers</strong>: F√°bio Cruz</p> <p><em>This blog post was developed while working at <a href="https://inductiva.ai/">Inductiva Research Labs</a>.</em></p> <hr/> <p>The world of classical simulation and the world AI are merging into a broader form of computational solutions for science and engineering. In this tutorial, we will guide you through an example one of the most exciting technical outcomes of this marriage between classical simulation and AI: Physics-Informed Neural Networks, or PINNs for short. The most notable difference between using PINNs vs using more generic neural networks is that, in PINNs, we explicitly encode knowledge about the the physical laws in the training loss of the neural network, allowing it to more quickly converge to functions that are physically sound.</p> <p>As a simple example, we will focus on the problem of üî• <strong>heat diffusion</strong> on a plate! üî•</p> <p>We will take you step by step up to a point where you can gauge the potential of Neural Networks in solving Partial Differential Equations:</p> <ol> <li> <p><strong>The Heat Equation</strong>: introduces the physics behind heat diffusion.</p> </li> <li> <p><strong>Finite-Differences</strong>: classical numerical routine to approximate its solution across a 2D plate.</p> </li> <li> <p><strong>Physics-Informed Neural Network</strong>: with that estimate of the solution as a <em>benchmark</em>, see how <strong>Machine-Learning algorithms</strong> fare for the same situation, by deploying a PINN (Physics-Informed Neural Network).</p> </li> <li> <p><strong>Generalized Neuro-Solver*</strong>: explore further into how versatile these Neural Networks are in handling <strong>more complex geometries</strong> than the 2D domain and <strong>varying initial/boundary conditions</strong>;</p> </li> </ol> <p>This field is fast-growing but, most importantly is at its <em>blissful infancy</em> stage. Our dream, rooted in a belief, is that Machine Learning may be refined to excel in extremely complex and customizable cases.</p> <p>This sophistication could potentially fare better in performance than every other classical solver, allowing us to go to lands classical frameworks have never granted us so far!</p> <p>Alright. üî• <em>Let‚Äôs get hot in here!</em> üî•</p> <h2 id="the-heat-equation-a-warm-up">The Heat Equation: a warm-up</h2> <p>Everybody has an elementary feeling for what heat is. <em>Heat diffusion</em> is the process through which energy is transported in space due to gradients in temperature.</p> <p>These considerations are reflected mathematically in the so-called <em>Heat Equation</em>, which sets how the temperature $u(t, \mathbf{r})$ at a point in space $\mathbf{r}$ and time instant $t$ evolves. It must satisfy:</p> \[\begin{equation} \frac{\partial u (t, \mathbf{r})}{\partial t} - D \Delta u(t, \mathbf{r}) = 0. \end{equation}\] <p>The thermal diffusivity $D$ controls how fast heat can spread around a neighborhood. For more realistic (and oftentimes less structured) media, their specifics result in a <em>local</em> diffusity $D(t, \mathbf{r})$. In the <em>Heat</em> series, we consider homogeneous media, for which $D$ is constant throughout.</p> <p>Now, what about $\Delta$ ‚Äî the <em>Laplacian</em> operator? Let‚Äôs see how it acts on a function $u$:</p> \[\Delta u = \nabla \cdot (\nabla u) = \sum_{i} \frac{\partial^2 u}{\partial {r_i}^2}\] <p>So we have $n$ $2^\rm{nd}$-order partial spatial derivatives to account for but also a single $1^\rm{st}$-order partial derivative in time. That‚Äôs a large batch of <strong>differential operators</strong> right there. We can lump them into a single <em>differential operator</em> which governs the evolution of the solution we‚Äôre after:</p> \[\mathcal{L} \left[ \frac{\partial}{\partial t}, \frac{\partial^2}{\partial^2_{r_1}}, \dots \frac{\partial^2}{\partial^2_{r_n}} \right] u(t, \mathbf{r}) = 0\] <p>Alright ‚Äî no doubt we‚Äôre in the presence of a <strong>Partial Differential Equation</strong>.</p> <p><em>These beasts are not easy to tame</em>. However, throughout the <em>Heat</em> series, we exploit the structural simplicity of the Heat Equation ‚Äî alongside the fact it is both very intuitive to understand and easy to set up meaningfully.</p> <h3 id="heating-across-a-2d-square-plate">Heating across a 2D square plate</h3> <p>We‚Äôll guide you on how to solve this equation in a simple yet realistic geometric setup: across a <em>2-dimensional plate</em> of variable size [2].</p> <p>As you will soon see, this domain is simple enough not only to allow intuitive visualization but also to provide the possibility of adding extra complexity without much effort.</p> <p>So let‚Äôs have a go. Our spatial vector in 2 dimensions is $\mathbf{r} = (x, y)$ and so the heat equation takes the form:</p> \[\left[ \frac{\partial}{\partial t} - D \left( \frac{\partial^2}{\partial x ^2} + \frac{\partial^2}{\partial y ^2} \right) \right]u(t,x,y)= 0\] <p>From a computational point of view, it will do us a favor to think in terms of the result upon applying each differential operator to the solution:</p> \[\begin{equation} u_t - D \left( u_{xx} + u_{yy} \right) = 0 \label{HE_2D} \end{equation}\] <p>where the partial derivatives are shorthanded to, say, $\partial_{x} \equiv \partial / \partial x$ and $\partial_{xx} = \partial_{x} \partial_{x}$.</p> <p>But hold on! We can‚Äôt start solving this beast just yet‚Ä¶</p> <h3 id="setting-boundary-and-initial-conditions">Setting boundary and initial conditions</h3> <p>We need more information to formulate completely this PDE. <strong>Initial and boundary conditions</strong> ensure that a <em>unique</em> solution exists by the function at particular points $(t,x,y)$ of the input space.</p> <p>These are normally reasoned through intuition and hindsight. Throughout the <em>Heat</em> series, we employ <em>Dirichlet boundary conditions</em>, which ensure:</p> <ul> <li> <p>through the <em>initial condition</em>, that a certain temperature is fixed across <em>all</em> points on the plate at the initial time instant (when $t=0$). We‚Äôll take $ u(0, x, y) = -1 ^\mathrm{o}C $.</p> </li> <li> <p>through the the <em>boundary conditions</em>, that a temperature is fixed at <em>all</em> times <em>only</em> for points along the 4 edges of the plate (top, bottom, left and right). We choose the energy source to act as to get the top edge to some hot temperature. We‚Äôll take $u(t, x, y_{\max}) = 1^\circ C.$ The remaining edges are held at some cold temperature. We‚Äôll take: \(\underbrace{u(t, x, y_{\min})}_{\text{Bottom}} = \underbrace{u(t, x_{\min}, y)}_{\text{Left}} = \underbrace{u(t, x_{\max}, y)}_{\text{Right}} = -1^\circ C.\)</p> </li> </ul> <div style="text-align: center;"> <img style="width:40%;" src="../../../assets/img/blogposts/heat1/BIC.png"/> </div> <p><strong>Fig. 1</strong>: The boundary and initial conditions used throughout the Heat series. Energy is pumped from the top edge onto an initially completely cold 2D plate. Credits: David Carvalho / Inductiva.</p> <h3 id="classical-numerical-methods">Classical Numerical Methods</h3> <p>As usual, finding governing equations from first principles is actually the easy part. Rather, solving them presents us major challenges. Why?</p> <p><strong>Most PDEs do not admit analytical, pen-and-paper solutions.</strong> Only a handful of cherry-picked differential operators can give rise to closed-form solutions. <em>Lesson learnt</em> ‚Äî <strong>numerical approximations</strong> must be used.</p> <blockquote> <p>But which ones? In which conditions? For what type of PDE?</p> </blockquote> <p>These are general and difficult questions to answer. Wildly different methods have been curated by the computationally-inclined in the Mathematical and Physical communities ‚Äî not as recipes set on stone but rather frameworks prone to constant scrutiny.</p> <p><em>Mesh-based methods</em> are traditionally the dominant approaches. The main idea is to <strong>discretize</strong> the domain of interest into a set of mesh points and, with them, approximate the solution. A whole zoo of methods exists, in particular:</p> <ul> <li><strong>Finite Differences Methods (FDMs)</strong> replace the PDE expression with a discretized analogue along a grid with the aid of <em>function differences</em>.</li> <li><strong>Finite Elements Methods (FEMs)</strong> subdivide the problem domain in multiple elements and apply the equations to each one of them.</li> <li><strong>Finite Volume Methods (FVMs)</strong> builds a solution approximation based on the exact computation (taking advantage of the divergence theorem) of the average value of the solution function in each of the smaller sub-volumes in which the domain is partitioned.</li> </ul> <p>All these methods have their advantages and shortcomings given the idiosyncrasies of each PDE and setup of the problem. Sometimes they may even coincide ‚Äî for some simple scenarios (such as regular grids) FEMs and FDMs <em>might</em> end up being the same [1].</p> <p>Typically, mesh-based classical routines tend to be very efficient in low-dimensional problems on regular geometries. We will then use this convenience to our advantage by applying <strong>a FDM in a FTCS (Forward in Time, Centered in Space) scheme.</strong></p>]]></content><author><name></name></author><summary type="html"><![CDATA[First blogpost of a series on the Heat equation, where we introduce the Heat partial differential equation.]]></summary></entry></feed>